<p>Targets is an R package that aids in writing performant and reproducible workflows. The Research Computing Platform (RCP) encourages you to use Targets where possible in writing analyses in R.</p>
<h2><span class="fontColorThemeSecondary">Training</span></h2><p>RCP have run a workshop on using Targets at WEHI, including modifications to the workshop to work on WEHI's Milton HPC system. To watch the recording or to browse the training materials, <a href="/sites/rc2/SitePages/RCP-Training.aspx">please visit the Training page</a>.</p><p>For general materials about how to learn Targets, please refer to the <a href="https://github.com/ropensci/targets#documentation">Targets GitHub page.</a></p>
<h2><span class="fontColorThemeSecondary">Targets on Milton HPC</span></h2><p>Here is a list of general advice for Targets users who want their workflow to run optimally on WEHI's HPC system:</p><h4>OnDemand</h4><p>As with all R workloads at WEHI, you are advised to use <a href="/sites/rc2/SitePages/R-services.aspx#ondemand-rstudio">RStudio OnDemand</a> to run Targets workflows.</p><h4><strong>Slurm Controller</strong></h4><p>Use the <a href="https://wlandau.github.io/crew.cluster/reference/crew_controller_slurm.html">crew_cluster_slurm</a> controller to ensure that Targets fully utilises the HPC capabilities. For example, you can add the following to your _targets.R to enable Slurm execution:</p><pre>tar_option_set(\n<span style="color:inherit;">  controller </span>=<span style="color:inherit;"> crew.cluster::</span>crew_controller_slurm(\n<span style="color:inherit;">    workers </span>=<span style="color:inherit;"> </span>3<span style="color:inherit;">,</span>\n<span style="color:inherit;">    script_lines </span>=<span style="color:inherit;"> </span>"module load R/4.2.3"<span style="color:inherit;">,</span>\n<span style="color:inherit;">    slurm_memory_gigabytes_per_cpu </span>=<span style="color:inherit;"> </span>1\n<span style="color:inherit;">  </span>)\n)</pre><p>If you encounter issues with the Slurm controller, please contact RCP. In some cases it may be necessary to fall back on the standard multiprocessing controller:</p><pre>tar_option_set(\n<span style="color:inherit;">  controller </span>=<span style="color:inherit;"> crew::</span>crew_controller_local(<span style="color:inherit;">workers </span>=<span style="color:inherit;"> </span>3)\n)</pre><p>However if you do this, ensure that the number of cores that you request in OnDemand corresponds to the number of workers you request:</p><div class="imagePlugin" style="background-color:transparent;position:relative;" data-alignment="Center" data-imageurl="/sites/rc2/SiteAssets/SitePages/R-Targets/2773686209.png" data-overlaytextstyles="{&quot;textColor&quot;:&quot;light&quot;,&quot;isBold&quot;:false,&quot;isItalic&quot;:false,&quot;textBoxColor&quot;:&quot;dark&quot;,&quot;textBoxOpacity&quot;:0.54,&quot;overlayColor&quot;:&quot;light&quot;,&quot;overlayTransparency&quot;:0}" data-imagenaturalheight="242" data-imagenaturalwidth="910" data-siteid="93ee3842-f21b-4d8d-b85b-874e64c67802" data-webid="95711e2c-a16f-49a8-9adc-1b9756d4ee5e" data-listid="3d75202a-c480-42f5-abb7-5e7b39fc27d5" data-uniqueid="cb7adb11-c89c-4ae1-b3d3-9dd21eed140d" data-height="123.19443746366191" data-width="463.2518102972411" data-widthpercentage="39.12496484338915" data-uploading="0"></div><h4>SBATCH Options</h4><p>If you want to configure the Slurm job that is used to run your Targets, for example to request more RAM, you should first look for <a href="https://wlandau.github.io/crew.cluster/reference/crew_controller_slurm.html">a corresponding argument to crew_controller_slurm</a>. For example, to request 100 GB of RAM (per CPU), you can use:</p><pre><span style="color:inherit;">crew.cluster::</span>crew_controller_slurm(\n<span style="color:inherit;">    workers </span>=<span style="color:inherit;"> </span>3<span style="color:inherit;">,</span>\n<span style="color:inherit;">    script_lines </span>=<span style="color:inherit;"> </span>"module load R/4.2.3"<span style="color:inherit;">,</span>\n<span style="color:inherit;">    slurm_memory_gigabytes_per_cpu </span>=<span style="color:inherit;"> </span>100\n)\n</pre><p>If an appropriate argument doesn't exist, then <span style="color:inherit;">script_lines argument to crew_controller_slurm </span><a href="https://github.com/wlandau/crew.cluster/discussions/12#discussioncomment-6403776"><span style="color:inherit;">is the recommended way to add SBATCH flags to your Slurm jobs</span></a><span style="color:inherit;">. Just make sure your SBATCH lines come <strong>before</strong> any bash commands such as module loading. For example, to run on a GPU node, you could use the following controller:</span></p><pre><span style="color:inherit;"> crew_controller_slurm(\n    name = "gpu_worker",\n    workers = 1,\n    script_lines = c(\n      "#SBATCH --partition=gpuq",\n      "#SBATCH --gres=gpu:1",\n      "module load R/4.2.3"\n    ),\n    slurm_memory_gigabytes_per_cpu = 1,\n    slurm_cpus_per_task = 1\n)</span></pre><h4>File Management</h4><p>By default, Targets puts its data store in the same location as your Targets workflow code. However this is not ideal for working on Milton. What we suggest is that you put your code in any permanent filesystem (ie not VAST Scratch), such as your home, lab share or a VAST Project. Then, you should set the Targets data store to use VAST Scratch. If you haven't already, you will need to <a href="https://support.wehi.edu.au/support/catalog/items/71">apply for access to Scratch</a>. Then, run the following code in your RStudio console<i> </i><strong>(do not put this code in your _targets.R).</strong> You will need to replace &lt;PROJECT NAME&gt; with the name of your current Targets workflow project.</p><pre>project_dir &lt;- file.path("/vast/scratch/users", Sys.info()["user"], "&lt;PROJECT NAME&gt;")<br>dir.create(project_dir, showWarnings = FALSE)<br><span class="fontSizeMedium">tar_config_set(store = project_dir)</span></pre><p style="margin-left:0px;">This will create a _targets.yml in your project directory if it doesn't exist, which will configure Targets to use VAST Scratch for its data store. Note that, because VAST Scratch is cleaned up every fortnight, your pipeline will possibly have to re-run from the start (without the cache) at this time. It is also possible that the cleanup will only partially remove your data store, in which case you will need to run tar_destroy("local") to properly clean it up, before re-running your pipeline.</p>
