<p>Dorado is an open-source software released and maintained by Oxford Nanopore Technologies that succeeds Guppy. It performs basecalling specifically for Nanopore sequencing signals as well as some post-processing steps. At time of writing, the latest version is 0.3.0.&nbsp;</p><p aria-hidden="true">&nbsp;</p><p>Dorado is available on Milton through the dorado modules. Dorado is capable on working on both CPUs and GPUs.</p><pre>$ module avail dorado\n----------------------------------------- /stornext/System/data/modulefiles/nvidia ------------------------------------------\ndorado/0.1.1-gcc-9.1.0&nbsp; dorado/0.2.1-gcc-9.1.0&nbsp; dorado/0.3.0\n</pre><h3><span class="fontColorThemeSecondary">Basecalling with Dorado</span></h3><p>Note that on Milton, Dorado is compatible with the A10, A30, and A100 GPUs; but not the P100s.</p><p aria-hidden="true">&nbsp;</p><p>The basecaller functionality is invoked with the command:</p><pre>dorado basecaller ${DORADO_MODELS}/&lt;model&gt; &lt;input-path&gt;\n</pre><p>where &lt;input-path&gt; is searched (non-recursively) for fast5 or pod5 files, and output results are printed to the terminal. To redirect the output to a file, add "&gt; outputfile.out" to the command. Dorado will automatically detect the hardware available and try to make use of it. For example, if you request 3 GPUs from Slurm, Dorado will make use of all those 3 GPUs without any further setup needed.<br>&nbsp;</p><p>DORADO_MODELS is the environment variable which is set by the module. This directory contains the &lt;model&gt;s used to translate signals into bases. For example</p><pre>dorado basecaller ${DORADO_MODELS}/dna_r10.4.1_e8.2_400bps_sup@v3.5.2 pea_DNA_pod5</pre><p>which uses the dna_r10.4.1_e8.2_400bps_sup v3.5.2 model.</p><p aria-hidden="true">&nbsp;</p><h4>Slurm Job Template</h4><p>To submit Dorado as a Slurm job, remember to request GPUs and submit to the gpuq partition:</p><pre>#!/bin/bash<br><br>#SBATCH --partition gpuq     # submitting to the gpuq partition<br>#SBATCH --cpus-per-task 12   # requesting 12 (logical) CPUs<br>#SBATCH --memory 64G         # requesting 64GB<br>#SBATCH --gres gpu:A30:1     # requesting 1 A30 GPU<br>#SBATCH --time 60            # requesting 60mins for the job<br><br>module purge<br><br># loading the dorado module<br># Remember to check for new versions on Milton with<br># module avail dorado<br>module load dorado/0.4.1<br><br># executing dorado. Replace basecaller with subcommand of interest<br>dorado basecaller $DORADO_MODELS/dna_r10.4.1_e8.2_400bps_sup@v4.2.0 /path/to/inputfiles &gt; /path/to/basecalledreads.txt</pre><p aria-hidden="true">&nbsp;</p><p>$DORADO_MODELS/dna_r10.4.1_e8.2_400bps_sup@v4.2.0 is referencing the dna_r10.4.1_e8.2_400bps_sup@v4.2.0 model which is stored in the $DORADO_MODELS variable set by the module. Change this model to whichever you like. You can check which models we have downloaded centrally with</p><p aria-hidden="true">&nbsp;</p><pre>$ module load dorado/0.4.1<br>$ ls $DORADO_MODELS<br>dna_r10.4.1_e8.2_260bps_fast@v3.5.2                dna_r10.4.1_e8.2_400bps_hac@v4.0.0_5mCG_5hmCG@v2<br>dna_r10.4.1_e8.2_260bps_fast@v3.5.2_5mCG@v2        dna_r10.4.1_e8.2_400bps_sup@v3.5.2<br>dna_r10.4.1_e8.2_260bps_fast@v4.0.0                dna_r10.4.1_e8.2_400bps_sup@v3.5.2_5mCG@v2<br>dna_r10.4.1_e8.2_260bps_hac@v3.5.2                 dna_r10.4.1_e8.2_400bps_sup@v4.0.0<br>dna_r10.4.1_e8.2_260bps_hac@v3.5.2_5mCG@v2         dna_r10.4.1_e8.2_400bps_sup@v4.0.0_5mCG_5hmCG@v2<br>dna_r10.4.1_e8.2_260bps_hac@v4.0.0                 dna_r10.4.1_e8.2_400bps_sup@v4.2.0<br>dna_r10.4.1_e8.2_260bps_sup@v3.5.2                 dna_r10.4.1_e8.2_400bps_sup@v4.2.0_5mCG_5hmCG@v2<br>dna_r10.4.1_e8.2_260bps_sup@v3.5.2_5mCG@v2         dna_r10.4.1_e8.2_400bps_sup@v4.2.0_5mC@v2<br>dna_r10.4.1_e8.2_260bps_sup@v4.0.0                 dna_r10.4.2_e8.2_4khz_stereo@v1.0<br>dna_r10.4.1_e8.2_400bps_fast@v3.5.2                dna_r9.4.1_e8_fast@v3.4<br>dna_r10.4.1_e8.2_400bps_fast@v3.5.2_5mCG@v2        dna_r9.4.1_e8_fast@v3.4_5mCG@v0<br>dna_r10.4.1_e8.2_400bps_fast@v4.0.0                dna_r9.4.1_e8_hac@v3.3<br>dna_r10.4.1_e8.2_400bps_fast@v4.0.0_5mCG_5hmCG@v2  dna_r9.4.1_e8_hac@v3.4_5mCG@v0<br>dna_r10.4.1_e8.2_400bps_hac@v3.5.2                 dna_r9.4.1_e8_sup@v3.3<br>dna_r10.4.1_e8.2_400bps_hac@v3.5.2_5mCG@v2         dna_r9.4.1_e8_sup@v3.4_5mCG@v0<br>dna_r10.4.1_e8.2_400bps_hac@v4.0.0                 rna003_120bps_sup@v3</pre><p>If a model is missing, request that it be downloaded by sending an email to <a href="mailto:support@wehi.edu.au">support@wehi.edu.au</a>.</p><p aria-hidden="true">&nbsp;</p><p>Note that Dorado will automatically try to use all the GPUs you requested in your Slurm job - no options needed!</p><p aria-hidden="true">&nbsp;</p><p>Please read the <a href="https://github.com/nanoporetech/dorado#dorado">Dorado README</a> for more detailed usage instructions.</p><p aria-hidden="true">&nbsp;</p><h4>GPU Compatibility</h4><p>Note that dorado is not compatible with Milton's older P100 GPUs, but they work on both our A100s and A30s. You can explicitly request an A30 or an A100 GPU like in the above template, or you can request an “Ampere” GPU by using the below SBATCH options:</p><pre>#SBATCH --gres gpu:1<br>#SBATCH --partition gpuq<br>#SBATCH --constraint Ampere</pre><p>This means that you will get an A30 GPU, or if all the A30 GPUs are being used, an A100.</p><p aria-hidden="true">&nbsp;</p><p>Note that we have a maximum of 4 A30 GPUs per node and 2 A100s per node.</p><p aria-hidden="true">&nbsp;</p><h4>Splitting large datasets</h4><p>Like Guppy, for large datasets, you can increase throughput by submitting multiple Slurm jobs using Dorado basecaller. This can be done by subdividing the fast5/pod5 input files and having each job work on these subdirecteries. This would mean that output files also need to be in different directories and joined afterward.</p><p aria-hidden="true">&nbsp;</p><p>During quiet periods, we recommend making use of the <a href="/sites/rc2/SitePages/SLURM-partitions.aspx#slurm-bonus-qos%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B">bonus QOS</a> to increase the number of GPUs your jobs can use at once.</p><p>&nbsp;</p><h3><span class="fontColorThemeSecondary">Converting fast5 to pod5</span></h3><p>The use of pod5 files is key to ONT's strategy to improving basecalling of nanopore reads. If you have existing fast5 files which you wish to convert to pod5, you can use the pod5 pip package. Install into your environment with</p><pre>pip install pod5</pre><p>and you can convert a directory of fast5 files into pod5 with</p><pre>pod5 convert from_fast5 &lt;input-dir&gt; --output &lt;output-dir&gt;</pre><p>The tool converted reads at about 50000 reads/s on the vc7-shared. This might take awhile if you've got a lot of data!</p><p>&nbsp;</p><h3><span class="fontColorThemeSecondary">Dorado Basecalling Performance</span></h3><p>&nbsp;</p><h4>How many CPUs and how much memory to request?</h4><p>The number of CPUs Guppy can utilise scales linearly with the number of GPUs, but also depends on the accuracy of the model being used. For SUP models run on Milton's A30 and A100 GPU nodes, a good ratio of CPUs to GPUs is 4:1. So, if you request 2 GPUs, you should pass --cpus-per-task=8 and if you request 3, then you pass --cpus-per-task=12 etc. This ratio should ensure that Guppy performance isn't bottlenecked by the number of CPU cores, while also not unnecessarily occupying too many of a node's available CPU cores. 50GB of memory per GPU should provide plenty of memory for both A30s and A100s (these are <i>very</i> conservative numbers to avoid your job failing). Lower accuracy models require greater CPU use as the CPU(s) are busier transferring data between the filesystem and the GPU(s).</p><p>&nbsp;</p><h4>Tuning Performance</h4><p>Unlike Guppy, Dorado will automatically test batch sizes to determine the optimal batch size to ensure a high throughput. For example, if I run (on one of Milton's A30 GPUs):<br>&nbsp;</p><pre>dorado basecaller -v ${DORADO_MODELS}/dna_r10.4.1_e8.2_400bps_sup@v3.5.2 pea_DNA_pod5 &gt; /dev/null</pre><p>noting that "-v" is for verbose information and "&gt; /dev/null" is redirecting output data to the void; I see the output:<br>&nbsp;</p><pre>[2023-06-07 16:46:18.512] [info] &gt; Creating basecall pipeline\n[2023-06-07 16:46:20.421] [debug] Auto batch size: GPU memory available: 24.35219456GB\n[2023-06-07 16:46:20.421] [debug] Auto batch size: testing up to 512 in steps of 64\n[2023-06-07 16:46:22.263] [debug] Auto batchsize: 64, time per chunk 28.7892 ms\n[2023-06-07 16:46:22.334] [debug] Auto batchsize: 128, time per chunk 0.554232 ms\n[2023-06-07 16:46:22.415] [debug] Auto batchsize: 192, time per chunk 0.4185227 ms\n[2023-06-07 16:46:22.499] [debug] Auto batchsize: 256, time per chunk 0.330036 ms\n[2023-06-07 16:46:22.564] [debug] Auto batchsize: 320, time per chunk 0.20149441 ms\n[2023-06-07 16:46:22.637] [debug] Auto batchsize: 384, time per chunk 0.19163199 ms\n[2023-06-07 16:46:22.712] [debug] Auto batchsize: 448, time per chunk 0.16716573 ms\n[2023-06-07 16:46:22.796] [debug] Auto batchsize: 512, time per chunk 0.164376 ms\n[2023-06-07 16:46:22.813] [debug] - set batch size for cuda:0 to 512</pre><p>Which shows the small tests Dorado does at the beginning of the run. In this case, dorado chose a batch size of 512, which is optimal for the default chunk size.<br>&nbsp;</p><p>&nbsp;</p><h3><span class="fontColorThemeSecondary">Beware of the version which you are using!</span><br>&nbsp;</h3><p>Dorado and the pod5 data format are in active development. Significant changes can occur between updates. For example, dorado basecalling in v0.3.0 is significantly faster than v0.1.0, but also that pod5 files created with older versions of the pod5 utility may not be compatible with newer versions of dorado. Noting the version can be very important for future reproducibility.<br>&nbsp;</p>
