<h2>Policies and SOPs</h2><p>RCP was engaged in the development of the following.</p>
<h2>Secure Data Platforms on the Cloud</h2><p>RCP supports different labs running workflows in the cloud. Each lab has different requirements for data access and processing and therefore there is no one-solution-fits-all. So far we have worked with the following projects/labs​​​​​​​</p><h4><span class="fontColorThemeSecondary">Bahlo Lab on Terra.bio</span></h4><p>Researchers in the Bahlo lab, are investigating the contribution of two different biological mechanisms to the development of Parkinson's disease (PD), using data provided by the Accelerating Medicines Partnership Parkinson's disease (<i>AMP PD</i>) programme.&nbsp;RCP developed a pipeline in Workflow Definition Language (WDL) to run on the Broad Institute's Terra cloud analysis platform.</p><p>This pipeline uses best practices such as docker images to ensure reproducibility and has been optimised to ensure efficient use of resources via process parallelism and selection of compute nodes. We have also deployed the pipeline to Terra, and have provided support to researchers as they applied&nbsp;it to analyse&nbsp;1809&nbsp;patient samples.</p><h4>&nbsp;</h4>
<h4><span class="fontColorThemeSecondary">Colonial Foundation Healthy Ageing Centre (CFHAC) Secure Cloud Platform</span></h4><p>In this project,&nbsp;<a href="https://www.wehi.edu.au/colonial-foundation-healthy-ageing-centre">CFHAC</a>, DiUS (third-party consultant), RCP and ITS Research Systems&nbsp;(RS) are working closely together to design and implement a cloud-based computational platform over a two-month period between February and April 2023.&nbsp; RCP supported the work throughout all project phases. They were engaged in the design sprints of the project to provide feedback and insights that ensure more groups/labs can make use of the platform in the future as complimentary to WEHI&nbsp;Milton.  &nbsp;We also supported the AWS accounts creation and reporting for billing&nbsp;and security purposes.&nbsp;&nbsp;</p><p>The cloud platform&nbsp;is&nbsp;developed&nbsp;to enable CFHAC's proteomics analysis of the ASPREE cohort to identify dementia biomarkers. The solution&nbsp;is&nbsp;intentionally designed&nbsp;to enable other WEHI research projects&nbsp;in the future; it&nbsp;can&nbsp;be adapted to meet&nbsp;different compliance requirements (Australian privacy laws,&nbsp;HIPAA, GDPR) and provide a range of bioinformatic tools.&nbsp;</p>
<h2><span class="sp-mseditorfix" data-mseditorfix="363d7011-83dd-4cc0-bdec-ea4af4df4dbb:UpSkilling">Upskilling</span></h2><p aria-hidden="true">&nbsp;</p>
<h2>Capability Building Program</h2>
<h2>Surveys</h2>
<h2>Community Building&nbsp;</h2>
<h2>Curated Atlas Query</h2><p>The Curated Atlas Query project has involved creating and providing the first version of the CELLxGENE human atlas, that is queriable at the single-cell level, with metadata harmonised across samples.</p><p>This database offers the ability to study cell biology across 28 million cells from different tissues belonging to numerous different patients with different ages and ethnicities from familiar platforms such as R and Python.</p><p>To get to this stage, RCP helped to define a format for storing RNA counts so that count data can be loaded into R without exceeding reasonable memory limitations. A similar principle was applied to the count metadata, which is provided as a DuckDB table that can filter and analyse metadata extremely quickly while also retaining a low memory footprint. RCP also lead the development of an R package and a WIP Python package that allows this harmonised database to be queried and accessed using idiomatic language APIs.</p><p>Finally, RCP assisted in determining and provisioning the optimal platform for hosting this data. Ultimately, the CuratedAtlasQuery was hosted on ARDC's Nectar research cloud, where the data is provided with high availability on the Swift Object Store.</p>
<h2>Policies and SOPs</h2><p>RCP was engaged in the development of the following.</p><ol><li><a href="/sites/rc2/SitePages/Requesting-Resources.aspx" data-interception="off" target="_blank" rel="noopener noreferrer">Requesting&nbsp;Resource for App hosting</a></li><li><a href="/sites/rc2/SitePages/Shiny-Apps-Hosting.aspx" data-interception="off" target="_blank" rel="noopener noreferrer">Shiny apps hosting</a></li></ol><p>RCP engages with researchers, ITS teams and different WEHI Committees to finalise the Policies and SOPs, which are eventually endorsed by the IT Governance Committee (ITGC).</p>
<h2>Policies and SOPs</h2><p>Previously, RCP was engaged in the development of the following.</p><ol><li><a href="/sites/rc2/SitePages/high-performance-storage.aspx" data-interception="off" target="_blank" rel="noopener noreferrer">High-performance storage</a>​​​​​​</li></ol>
<h2>The Portable Pipelines Project (Janis)</h2><p>This project developed out of need to run (cancer) pipelines across WEHI, Peter Mac and Melbourne Bioinformatics. There is a plethora of pipeline tools but at that time most of the tools had not kept pace with the revolution in both the complexity of data and the complexity of compute environments. New tools and initiatives, for example Common Workflow Language (CWL), were being developed to address this complexity. However, it was clear this complexity was forcing tool developers to narrow their focus on an aspect of the pipeline domain. For example, CWL focused on providing a rigorous and complete description of a workflow but that the cost of ease of use and implementation support. Other tools were developed to interface with cloud and batch systems but at the expense of portability to other execution environments. At WEHI, we recognised that there was a need for APIs that would allow different aspects of a pipeline system to be coupled together dynamically. This architecture will greatly aid portability as it allows decoupling if the pipeline definition from specific infrastructure. A proof of concept was funded by Tony Papenfuss.&nbsp;</p><p>This architecture was adopted by the Portable Pipeline Group, funding was provided by Melbourne Bioinformatics and Peter Mac and Janis Project was born. Janis provides Python APIs for workflow definition, translation to transport formats (CWL, Broad’s WDL and Nextflow currently implemented) and execution (SLURM, PBS, AWS and GCP currently implemented). Specifying pipelines using the Python API is significantly easier than using current tools and pipelines have been demonstrated to run across precinct facilities and in the cloud.&nbsp;&nbsp;</p>
<h2>Benchmarking a Bayesian Computation Tool&nbsp;</h2><p>RCP collaborated with Stefano Mangiola, in the Papenfuss lab, to automate a parameter sweeps for an R package he developed. The package helps identify outlier data in differential expression analysis. These are very computationally expensive calculations which means that it is worth investing effort in understanding how different parameter choices and inputs affect execution time. We were able to help by automating the parameter sweeps. As Stefano modified his algorithms, we were easily able to rerun the benchmarks. We developed Python scripts to generate R scripts, submit to the batch system and then parse results from the R output.&nbsp;</p>
<h2>Three Thousand Malaria Genomes</h2><p>To better understand the evolution, and in particular the development of drug resistance, of plasmodium falciparum around the world, the Bahlo lab asked us to run a pipeline across 3000 WGS pf samples. They provided a <a href="http://docs.bpipe.org/">Bpipe</a> pipeline as a template however Bpipe does not scale to this amount of data. We redeveloped the pipeline using <a href="https://toil.ucsc-cgl.org/">Toil</a> which was the state-of-the-art workflow manager at the time. Toil did not support the Torque batch (which we were running at the time) so we submitted a pull request to add support for traditional batch systems. We were able to process ~3000 WGS sample through an 18-step workflow in a few weeks on Milton. The workflow was written in the Toil API and therefore is not of general interested but if would like to look at it please contact us.&nbsp;</p>
<p>RCP support for Advanced Genomics Facility is an ongoing project. So far we have been engaged in the following&nbsp;</p><h4><span class="fontColorThemeSecondary">MegaQC</span></h4><p>MegaQC is a database and web application used to monitor quality control information derived from next-generation sequencing technologies. Although it is common for sequencing facilities to calculate quality control data per sequencing batch using MultiQC, it is not common or easy to track this information between multiple runs.</p><p>At WEHI, the Advanced Genomics Facility will be able to use MegaQC to catalogue sequencing metadata and look for failing samples or unusual trends in the data.</p><p>So far, RCP has provided ongoing maintenance for the MegaQC package, along with supervising&nbsp;a student intern to develop new features for the project. Through this, the project has gained the ability to use a dependency lock file, to ensure MegaQC remains stable in the future, and the ability to calculate multivariate distances, which summarise how unusual each sequencing sample is using multiple different quality metrics simultaneously.</p><h4><span class="fontColorThemeSecondary">​​​​​​​Data Management and Transfer to Internal and External sites</span></h4><p>Currently, we are supporting the facility&nbsp;in developing an easy method for data movement. We are engaged with ITS and the facility to update and enhance their data management procedures.&nbsp;<br>&nbsp;</p><p><span class="fontColorThemeSecondary">​​​​​​​</span></p>
<p><span><span></span></span>RCP support for Cryo-EM is an ongoing project. So far we have been engaged in the following&nbsp;<br></p><h4><span class="fontColorThemeSecondary">Benchmarking and tuning Cryo-Em software project</span><br></h4><p><span><span>This project started late in 2020, to understand the reason behind the difference in performance between CryoSPARC on Milton and on Monash M3 Massive which researchers were observing.&nbsp;&nbsp;Benchmarking Cryosparc on both showed that the filesystem was the bottleneck and in collaboration with ITS, RCP performed another performance benchmark using the newly added VAST system which showed a huge boost in performance. The next step of the project was benchmarking Relion and providing documentation on how to use it with VAST. </span></span>The project delivered documentation for other tools to help structural biologists get started on Milton.</p><p>The technologies used are bash script and python.</p><h2><span class="fontColorThemeSecondary">CryoEM Milton Module</span><br></h2><p>This project was initiated March 2023, as a continuation of our support to the Structural Biology division. We are helping enhance the set of most commonly used tools and make them available via the modules system on Milton<br></p>
<p>Oxford Nanopore Technologies (ONT) tools, such as Megalodon and Guppy, are used by multiple labs. RCP was approached for advice on which parameters maximised performance on Milton's GPUs. So we performed a parameter scan for&nbsp;Megalodon and Guppy basecallers. And furthermore, the scan&nbsp;expanded into investigating the in-development dorado basecaller and compared performance with Guppy. You can find results shared <strong><a href="/sites/rc2/SitePages/ONT-Guppy.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/ONT-Guppy.aspx" target="_blank" data-interception="off" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/ONT-Guppy.aspx">here</a>.</strong></p><p>We&nbsp;provided a recommendation to researchers about Guppy and megalodon run-time parameters, and in the process, we have made a contribution to the broader community by participating in a&nbsp;dorado performance issue investigation.<span>​​​​​​​</span></p>
<p>RCP runs a lot of benchmarking tests&nbsp;and parameter scans, as part of supporting researchers' workloads. There was a need for an easy-to-use and streamlined tool that can be easily configured to benchmark&nbsp;any tool. We designed this tool so that it can be easily run on any SLURM cluster.&nbsp;<br>&nbsp;</p><p>This tool can also be used by researchers,&nbsp; to help them answer the&nbsp;challenging question about how many resources their jobs&nbsp;need without wasting resources and also how the amount of&nbsp;resources scales with the size of the input dataset. This is especially useful for jobs that are repeatedly run.&nbsp;​​​​​​​</p>
<p>RCP encourages researchers to use workflow managers than running their pipelines manually. Our support includes running training sessions, and providing advice on how to design your workflow and we also partner with researchers to develop workflows that fit&nbsp;their research. This project is an ongoing project and it made up of multiple sub-projects.</p><h4><span class="fontColorThemeSecondary">BioNix&nbsp;</span></h4><p>This sub-project is a close engagement and collaboration with Justin Bedo and the Stafford Fox project team (Papenfuss Lab) to support their BioNix workflow running on Milton. RCP has worked closely with Justin and ITS RS to optimise VAST storage and resolve issues arising from the asynchronous nature of NFS filesystems. As part of this project, multiple student internships have&nbsp;worked on testing and documenting BioNix on Milton. This will&nbsp;make it more accessible to other researchers in WEHI.</p><h4><span class="fontColorThemeSecondary">NextFlow&nbsp;</span></h4><p>RCP had engaged with Australia Biocommons to provide&nbsp;Nextflow Tower Service to WEHI researchers.&nbsp;Our support includes running training sessions, and providing advice on how to design your workflow and also providing documentation.</p>
<p>For very large datasets, the size of the data&nbsp;introduces a storage challenge. Large datasets also introduce an engineering problem. When downloading a relatively small dataset, it is reasonable to handle occasional failures manually. For large datasets, failures are inevitable and manual handling of errors becomes impractical. We have previously developed&nbsp;two&nbsp;solutions for handling downloads from Globus and GDC (TCGA). The software takes care of driving the query and download APIs, parallelisation, error handling and retries. When a sample is downloaded a user workflow is called. If the workflow succeeds that sample can be deleted. This allows decoupling of the engineering problems of handling the data from the process of analysing the data.&nbsp;<br></p><p>We have extended the project in 2021 to be more generic and flexible. Our objective was to make it easily extendible to use&nbsp;any external organisations or tool for download. The new DPD pipeline&nbsp;uses&nbsp;a config file that details the data to download, locations for files, logs and scripts as well as other information needed for the pipeline to run.&nbsp;The pipeline is written in Python and utilises md5 checks and SLURM on_success or on_failure qualifiers to handle errors and send the data further down the pipeline to the subsequent stage.&nbsp;</p><p>The project will be taken further and be migrated to NextFlow as a workflow manager.&nbsp;&nbsp;This will help make it more portable and easily extended to other data sources and data processing techniques.</p><p>The technologies used so far were python and bash scripting</p>
<p>This project was to make the&nbsp;code for protein hallucination and&nbsp;inpainting described in&nbsp;<a data-cke-saved-href="https://www.biorxiv.org/content/10.1101/2021.11.10.468128v2" href="https://www.biorxiv.org/content/10.1101/2021.11.10.468128v2">this preprint</a>&nbsp;available to run on Milton.&nbsp;&nbsp;We have all the conda envs required to run AlphaFold and Rosetta code setup in addition to pymol and psipred. Due to licensing, the git repo is not public but if you want to try it for your research, please email <strong><a href="mailto:research.computing@wehi.edu.au" data-cke-saved-href="mailto:research.computing@wehi.edu.au" data-interception="off" title="mailto:research.computing@wehi.edu.au" target="_blank">Research Computing</a></strong>.</p><p>The technologies used were bash scripting and python.</p>
<p>AlphaFold 2 nature paper was released in 2021, and RCP in collaboration with ITS RS provided a local copy on Milton for researchers to run. The <span class="fontColorThemeSecondary">AlphaFold on Milton</span> project was divided into two parts.</p><p>The first part was&nbsp;making available AlphaFold2 as a tool to run on Milton, we optimised the run by running it in different jobs with different requested resources. So that no resources are wasted. And also, an easy and simple command line interface was provided to help researchers with limited experience in HPCs.&nbsp;</p><p>The second part was downloading the AlphaFold protein structure database to a share location on VAST, so that it is accessible to all scientists and processing it into a format used by Dali for &nbsp;protein structural alignment and structure database search.</p><p>The technologies used were Singularity containers, bash scripting and python<br></p>
<p>The Milton Dashboard project delivered multiple dashboards for Milton users who use SLURM. The dashboards included&nbsp;</p><ol><li>Filesystems Utilisation</li><li>GPUs Utilisation</li><li>SLURM resources used and current jobs state</li><li>A user SLURM queue state</li></ol><p>The technologies used were Grafana, Prometheus, and GO.</p>
<p><a href="https://bioinf.wehi.edu.au/"><span class="fontColorThemeSecondary">https://bioinf.wehi.edu.au/</span></a></p><p>The old bioinformatics website was hosted on old infrastructure that did not pass security audits and required an upgrade of hardware and software. So, it was the perfect time to give the it a facelift. RCP was engaged to look into migrating to a secure https hosting and providing an easy, robust way for administrators who regularly update the site contents.</p><p>This was achieved by moving the website contents over to a Git repo and developing a&nbsp;CI/CD, continuous integration/continuous deployment pipeline connected to an AWS instance. This eliminates the need for users to SSH into the host server and add files through the CLI.&nbsp;Larger data files that are linked to the website were not suited for storage on GitHub. They were moved to an AWS S3 bucket.&nbsp;</p><p>Having the entire architecture built in AWS allowed&nbsp;us to utilise&nbsp;AWS CDK (cloud development kit). Using this infrastructure as code,&nbsp;we defined the web server contents,&nbsp;the&nbsp;security requirements and the CI/CD pipeline in code which can simply be run to bootstrap and deploy every component of the website. This allows for a quick reboot of the website should it ever be taken down and makes it easy to migrate to a different cloud provider if required.&nbsp;</p><p>The technologies used were AWS CDK, python, HTML, perl, and&nbsp;CGI<br>&nbsp;</p>
