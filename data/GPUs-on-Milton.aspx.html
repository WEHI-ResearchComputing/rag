<h2><span class="fontColorNeutralDark">GPUs on Milton</span></h2><p>Milton users can have access to the following GPUs via Slurm queues (<a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx">batch</a> and <a href="/sites/rc2/SitePages/Interactive-workloads.aspx#slurm-interactive-sessions">interactive jobs</a>)&nbsp;or <a href="/sites/rc2/SitePages/Open-OnDemand.aspx">Open OnDemand Desktop and Open OnDemand&nbsp;Jupyter notebook</a>.</p><p>If you have any enquiries about the GPUs, please email <a href="mailto:research.computing@wehi.edu.au">Research Computing</a>.</p>
<h4><span class="fontColorBlue">Contents on this page:</span></h4><ul><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#gpus-on-milton"><span class="fontColorThemeSecondary">GPUs on Milton</span></a></li><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#choosing-a-gpu">Choosing a GPU</a></li><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#using-gpus">Using GPUs</a></li><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#example">Example</a></li><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#gpu-dashboards">GPU dashboards</a></li><li><a href="/sites/rc2/SitePages/GPUs-on-Milton.aspx#requesting-the-a100-in-your-submission-script">Requesting the A100 in your submission script</a></li></ul>
<figure class="table canvasRteResponsiveTable tableCenterAlign" style="width:84.88%;" title="Table"><table class="bandedRowTableStyleTheme ck-table-resized"><colgroup><col style="width:14.82%;"><col style="width:14.82%;"><col style="width:11.03%;"><col style="width:9.93%;"><col style="width:16.04%;"><col style="width:16.83%;"><col style="width:16.53%;"></colgroup><tbody><tr><td>SLURM partition</td><td><p style="text-align:center;">GPU, GPU RAM</p></td><td><p style="text-align:center;">Nodes</p></td><td><p style="text-align:center;">GPUs per node</p></td><td><p style="text-align:center;">Resources<br>per node</p></td><td>Max GPUs<br>per user</td><td>Max jobs<br>per user</td></tr><tr><td><strong>gpuq</strong></td><td><p style="text-align:center;">A100, 40GB</p></td><td><p style="text-align:center;">3</p></td><td><p style="text-align:center;">2</p></td><td>96 Cores, 994GB</td><td><p style="text-align:center;">1</p></td><td><p style="text-align:center;">1</p></td></tr><tr><td><strong>gpuq</strong></td><td><p style="text-align:center;">A30, 24GB</p></td><td><p style="text-align:center;">7</p></td><td><p style="text-align:center;">4</p></td><td>96 Cores, 490GB</td><td><p style="text-align:center;">8 across 2 nodes</p></td><td><p style="text-align:center;">Up to GPU limits</p></td></tr><tr><td><strong>gpuq_interactive</strong></td><td><p style="text-align:center;">A10, 24GB</p></td><td><p style="text-align:center;">1</p></td><td><p style="text-align:center;">4</p></td><td>48 Cores, 238GB</td><td><p style="text-align:center;">1</p></td><td><p style="text-align:center;">1</p></td></tr><tr><td><strong>gpuq</strong></td><td><p style="text-align:center;">P100, 12GB</p></td><td><p style="text-align:center;">5</p></td><td><p style="text-align:center;">4</p></td><td>48 Cores, 113 GB</td><td><p style="text-align:center;">8 across 2 nodes</p></td><td><p style="text-align:center;">Up to GPU limits</p></td></tr></tbody></table></figure><p aria-hidden="true">&nbsp;</p><p>There is also an interactive node <span class="fontColorBlue">vc7-gpu-055 </span><span class="fontColorNeutralDark">which you can ssh to and contains</span>:<br>&nbsp;</p><ul><li>1 x M60</li><li>cgroups similar to vc7-shared.</li></ul><p>However, the performance of Milton's P100/A10/A30/A100 GPUs will substantially surpass the GPU on this node.<br>&nbsp;</p>
<p>Due to the problematic&nbsp;architecture of <span class="fontColorBlue"><strong>cryoSPARC</strong></span>, where all jobs are run using cryosparc user, not the real user that is running it,&nbsp;&nbsp;resources limits stated above will be overridden with a QOS that applies only to <span class="fontColorBlue">cryosparc user</span> such that</p><ul><li>Max resources allocated<ul><li>gpu=16, node=4</li><li>Time-Limit =48:00:00</li></ul></li></ul><p>CryoSPARC will have access only to P100</p>
<h2><span class="fontColorThemeSecondary">Choosing a GPU</span></h2><p>The following table helps users to choose a GPU, how many resources are available, and how to access this GPU.</p><p><span class="fontColorBlue"><strong>Always</strong></span> start by using <span class="fontColorBlue">P100</span> and if your jobs outgrow&nbsp;<span class="fontColorBlue">P100</span>, (i.e. they require more than 48 hours to run, or more RAM than is available) then try using <span class="fontColorBlue">A30</span>. If your job outgrows <span class="fontColorBlue">A30</span> and you can demonstrate that this job can efficiently use the resource, you can request access to <span class="fontColorBlue">A100</span>.</p><p>When using multiple nodes (on the<span class="fontColorBlue"> gpuq partition</span>) make sure your code supports <span class="fontColorBlue">multi-nodes.</span></p><figure class="table canvasRteResponsiveTable tableCenterAlign" style="width:81.59%;" title="Table"><table class="bandedRowTableStyleTheme ck-table-resized"><colgroup><col style="width:6.35%;"><col style="width:38.14%;"><col style="width:31.15%;"><col style="width:24.36%;"></colgroup><tbody><tr><td><p style="text-align:center;">GPU</p></td><td><p style="text-align:center;">When?</p></td><td><p style="text-align:center;">How much?</p></td><td><p style="text-align:center;">How?</p></td></tr><tr><td><strong>P100</strong></td><td><ul><li>Considered outdated, but still useable for most types of work.</li><li>If you are using multiple nodes, please make sure your code/tool supports them.</li><li>Commonly used on Milton for Relion and Cryosparc jobs.</li></ul></td><td><span class="fontSizeMediumPlus">5 nodes, 4 x P100/node,</span><br><span class="fontSizeMediumPlus">12GB RAM/GPU, 108GB RAM/node,</span><br><span class="fontSizeMediumPlus">48 CPU core/node</span></td><td><ul><li>SLURM jobs<br>(SBATCH and SALLOC)</li><li>OnDemand Jupyter notebook</li></ul></td></tr><tr><td><strong>A10</strong></td><td><ul><li>For visualisation software.</li></ul></td><td><span class="fontSizeMediumPlus">1 node, 4 x A10/node,</span><br><span class="fontSizeMediumPlus">24GB RAM/GPU, 256GB RAM/node,</span><br><span class="fontSizeMediumPlus">48 CPU core/node</span></td><td><ul><li>OnDemand desktop</li><li>SLURM jobs (SBATCH and SALLOC)</li></ul></td></tr><tr><td><strong>A30</strong></td><td><ul><li>The recommended GPU for most workflows</li><li>If you are using multiple nodes, please make sure your code/tool supports them.​​​​​​​</li></ul></td><td><span class="fontSizeMediumPlus">7 nodes, 4 x A30/node,</span><br><span class="fontSizeMediumPlus">24GB RAM/GPU, 512GB RAM/node,</span><br><span class="fontSizeMediumPlus">96 CPU core/node</span></td><td><ul><li>SLURM jobs<br>(SBATCH and SALLOC)</li><li>OnDemand Jupyter notebook</li></ul></td></tr><tr><td><strong>A100</strong></td><td><ul><li><span class="fontSizeMediumPlus">For workloads that require significant GPU memory.</span></li><li><span class="fontSizeMediumPlus">Should be reserved for jobs that demonstrate their scalability.</span></li></ul></td><td><span class="fontSizeMediumPlus">3 nodes, 2 x A100/node,</span><br><span class="fontSizeMediumPlus">40GB RAM/GPU, 1TB RAM/node,</span><br><span class="fontSizeMediumPlus">96 CPU core/node</span></td><td><ul><li>SLURM jobs<br>(SBATCH and SALLOC)</li></ul></td></tr></tbody></table></figure>
<p>If your software can use multiple GPUs, it may perform better with the A30 GPU nodes as each node has 4x A30s, which may perform better than a single A100!</p>
<h2><span class="fontColorThemeSecondary">Using GPUs</span></h2><p>To request GPUs in your Slurm submission script, you will need to set <i><span class="fontColorBlue">--partition </span></i>(<i><span class="fontColorBlue">-p</span></i>) and <i><span class="fontColorBlue">--gres.&nbsp;</span></i></p><p>If you have never used SLURM before, please read our <a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx">Getting started with SLURM guide</a> first.</p><h4><span class="fontColorBlue">Example</span></h4><pre>#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:P100:4</pre><p>The above example will allocate 4 x P100 from the&nbsp;<span class="fontColorBlue"><strong>gpuq</strong></span> partition. The number 4 represents the number of GPUs requested.</p>
<p><span class="fontColorThemeSecondary">When specifying --gres gpu:N, your job will be assigned to ANY of the A30, A100, or P100 GPUs based on availability.&nbsp;</span></p><p><span class="fontColorThemeSecondary">For example, to request 4xGPUs on a single node, you can use the following in your script:</span></p><pre>-p gpuq --gres gpu:4</pre>
<p>To request an <span class="fontColorBlue"><strong>A30</strong></span>, use the following Slurm directives:</p><pre>#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A30:1</pre><p>To request an <span class="fontColorBlue"><strong>A10</strong></span> from the <span class="fontColorBlue">gpuq_interactive</span>, use the following Slurm directives:</p><pre>#SBATCH --partition=gpuq_interactive\n#SBATCH --gres=gpu:A10:1</pre><p>To request an <span class="fontColorBlue"><strong>A100</strong></span> from the <span class="fontColorBlue">gpuq</span> partition, use the following Slurm directives:</p><pre>#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100:1\n</pre>
<h2><span class="fontColorThemeSecondary">GPU dashboards</span></h2><p>There are <a href="http://dashboards.hpc.wehi.edu.au/d/vBsWuLJ7z/gpus?orgId=1"><span class="fontColorThemeSecondary">GPU dashboards</span></a> that track the current utilisation of the GPUs and thus help you decide which GPU to use for your job so you can avoid long wait times.</p>
<p><strong>GPUs are also available through interactive Slurm sessions.</strong></p><h4><span class="fontColorBlue">Example:</span></h4><pre>salloc -p gpuq_interactive --gres gpu:A10:1 --time 1:00:00 --mem 1G --cpus-per-task 5 --x11</pre><p>This will request an interactive session on the <span class="fontColorBlue">gpuq_interactive</span> partition, using 1 x A10 with a one-hour time limit and 1GB of memory. To be able to open visualisations, do not forget <span class="fontColorBlue"><strong>--x11</strong></span><span class="fontColorNeutralDark">.</span></p>
