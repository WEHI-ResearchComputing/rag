<h2><strong>Implementing bonus --requeue functionality</strong></h2><ul><li><a href="/:w:/r/sites/ResearchComputingSteeringGroup/Shared%20Documents/Papers/Slurm%20review%202022/SLURM%20Review.docx?d=wb249664851a147e38d1c5893ac228ade&amp;csf=1&amp;web=1&amp;e=4M5zVB">When bonus was first implemented</a>, requeue was disabled and users were expected to manually resume pre-empted jobs</li><li>We are currently exploring alternative strategies to prevent the resubmission of faulty jobs that cause node failures or consistently do not execute successfully.</li><li>Nextflow users can ask Nextflow to “retry” their jobs, but pure Slurm script users cannot, which is unnecessarily restrictive.</li></ul><h2><strong>Increasing bonus maximum job limits</strong></h2><ul><li>This will increase throughput on bonus and make it similar to the regular queue</li></ul><h2><strong>Increasing VAST Scratch deletion duration</strong></h2><ul><li>Survey feedback shows that most respondents preferred 30 days.</li><li>This is aligned with other HPC facilities.</li><li>May be associated with lowering the VAST Scratch quota (currently 100TB).</li></ul><h2><strong>Changes to the GPU nodes</strong></h2><ul><li>The limit on GPU nodes will be changed from “8 A30/P100 across two nodes”, to “8 A30/P100 and a memory limit of 1TB”.<ul><li>This enables users to make full use of the 8 GPU limit. The memory limit will ensure that users cannot request more than two nodes worth of memory.</li></ul></li><li>The A30 and P100 GPUs will be available to the Open OnDemand <u>Code Server</u>.</li></ul><h2><strong>New partition</strong></h2><ul><li>A <u>datamover</u> partition will be added for data transfer. <span class="fontColorRedDark"><strong>The partition limits will be 16 CPU cores, 64GB, 1-week timelimit.</strong></span> This was approved in last year’s Slurm review, but not implemented. The main uses for this are:<ul><li>For long-running data transfer jobs</li><li>Facilitate the migration to Stornext Read Only</li></ul></li></ul><h2><strong>Stornext Read Only</strong></h2><ul><li>The Slurm survey indicates that users would prefer this be done together with the OS upgrade (<strong>Thursday 15 to Monday 19 August</strong>). Suggestions on how best to do this will be appreciated. The motivation for the change is as follows:<ul><li>Stornext is prone to failover (i.e., requiring restart) in the case of undesirable write patterns (e.g., high-frequency unbuffered writes). Additionally, it is used by everyone in WEHI (e.g., professional service, lab staff).<ul><li>The read-only move will significantly reduce likelihood of failure and interrupting WEHI operations.</li></ul></li><li>Stornext is not designed as a parallel filesystem, and thus cannot handle many concurrent writes<ul><li>The read-only move will reduce Slurm jobs’ run times as less time is spent waiting for IO</li></ul></li><li>Stornext backs up all files nightly (with some exceptions). This process incurs costs for WEHI in terms of finances, time, and storage space needed to purchase, maintain, and permanently house these tape drives.<ul><li>The read-only move will reduce the likelihood of unnecessary files being stored and reduce operation cost of Stornext.&nbsp;</li></ul></li></ul></li></ul><p style="margin-left:32.05pt;">Making Stornext “read only” involves making all paths within the /stornext directory read-only from all compute nodes. <span class="fontColorRedDark"><strong>Login nodes (slurm-login and vc7-shared) will not be affected.</strong> <strong>Home directories will remain writable, e.g. when programs create/modify config files.</strong></span></p><p style="margin-left:32.05pt;">This means that all Slurm jobs running on the following partitions <span class="fontColorRedDark"><strong>(regular, bigmem, long, interactive, gpuq, gpu_interactive) </strong></span>will not be able to write/save files to any subdirectory of <span class="fontColorBlue"><strong>/stornext</strong></span>, except the user’s HOME directory. The VAST filesystem will be accessible for writing and saving files.&nbsp;</p><p style="margin-left:32.05pt;">Alternatively, users can use the <span class="fontColorRedDark"><strong>datamover</strong></span> partition and login nodes to do any writes to Stornext. Writes on the datamover partition will require a separate job/process/task in your workflow/pipeline that moves only files required for archiving into Stornext.</p><p style="margin-left:32.05pt;">&nbsp;To assist non-CLI users, an OnDemand App will be provided to create and copy a tar-ed and compressed copy of folders from VAST to Stornext.</p><h2>RC will also provide the following to help make the transition smooth:</h2><ol><li>Make the datamover partition available for testing.</li><li>Make a node with the OS upgrade and Stornext RO that users can SSH to test scripts and pipelines.</li></ol>
<h3 style="margin-left:0px;">Changes to Posit Workbench and SLURM</h3><h4 style="margin-left:0px;">Posit Workbench (aka RStudio Server Pro)</h4><p style="margin-left:0px;">The memory limit per user will be decreased to 24 GB. This is to avoid server restarts due to memory overload. W<span class="fontSizeLarge">e strongly advocate shifting to the </span><a href="https://ondemand.hpc.wehi.edu.au/pun/sys/dashboard/batch_connect/sys/rstudio_latest/session_contexts/new"><strong><u>RStudio OnDemand</u></strong></a><span class="fontSizeLarge"> platform. If you are facing any issues using Open OnDemand, please do not hesitate to contact the </span>helpdesk (<a href="mailto:support@wehi.edu.au" data-interception="off" target="_blank" rel="noopener noreferrer"><strong><u>support@wehi.edu.au</u></strong></a>).</p><h4 style="margin-left:0px;">Open OnDemand RStudio</h4><p style="margin-left:0px;">The time limit for Open OnDemand RStudio sessions will be increased to 14 days when choosing the long partition.</p><h4 style="margin-left:0px;">Slurm GPU partitions</h4><ol><li>Consolidate GPU Partitions: Integrate the gpuq_large partition with the gpuq partition. This ensures that all GPUs are housed within a single partition, streamlining accessibility.&nbsp;</li><li>Optimize GPU Workflow: To encourage maximal use of all GPUs, we'll remove the default GPU setting (which is currently set to the P100). This change will permit workflows to engage with any available GPU within the partition.&nbsp;</li><li>Prioritize Newer GPUs: Increase the weights assigned to our newer GPU nodes. This ensures that non-specific requests are more inclined to employ the latest GPU models.&nbsp;&nbsp;</li></ol><p style="margin-left:0px;">It's important to note a potential downside to these changes. Workflows currently operating on P100 GPUs, specifically optimized for these GPUs, might face disruptions. Such issues could arise if the code is tailored for Tesla GPUs and isn't compatible with Amperes, or vice versa. In these cases, modifications will be required to explicitly specify the intended target GPUs.&nbsp;</p><p style="margin-left:0px;">If you have any issues with these changes or inquiries, please don’t hesitate to contact Research Computing.</p>
<p>RCP has just finalised an engagement and consultation round with the Milton users community, to discuss changes to SLURM partitions and configuration, especially with respect to the addition of the newly purchased nodes. The engagement included the following.</p><ol><li>Sharing&nbsp;results of user acceptance testing.</li><li>Sharing Milton 2023 survey (<a href="https://forms.office.com/Pages/AnalysisPage.aspx?AnalyzerToken=SgJDsHDflWO3TIvnPrSZn9aon3zSHAUP&amp;id=4nJ8qs_FQEqiKqWFwEygfK20cah37mBEtCZAZZAP9WBUQjNaV1NJRDgxRDJJSzVUUUEyRUZFWjAwTy4u"><span class="fontColorBlue"><strong>summary here</strong></span></a>) to gather feedback from users.&nbsp;</li><li>Open discussion with MUG on&nbsp;proposed changes to SLURM&nbsp;</li><li>Discuss, update and get the changes endorsed by RCP Advisory Group.</li></ol><p>Details are available in the document below and also in <a href="/sites/mug/SitePages/TrainingHome.aspx"><span class="fontColorBlue"><strong>MUG shared documents</strong></span></a><span class="fontColorBlue"><strong>.</strong></span>​​​​​​​</p><p>The new partition limits will be as follows, the GPU nodes will not be affected.</p><figure class="table canvasRteResponsiveTable tableCenterAlign" style="width:1019px;" title="Table"><table class="filledHeaderTableStyleTheme ck-table-resized"><colgroup><col style="width:12.6%;"><col style="width:23.39%;"><col style="width:19.98%;"><col style="width:22.93%;"><col style="width:21.1%;"></colgroup><tbody><tr><td><br> &nbsp;</td><td colspan="1">regular &nbsp;</td><td colspan="1">interactive &nbsp;</td><td colspan="1">long &nbsp;</td><td colspan="1">bigmem &nbsp;</td></tr><tr><td>Max Jobs Submitted per User  &nbsp;</td><td>5000 </td><td>1&nbsp;</td><td>5000</td><td>5000</td></tr><tr><td>Max Resources per User&nbsp;</td><td>cpu=454, mem=3TB</td><td>cpu=16,&nbsp;mem=64GB&nbsp;</td><td>cpu=96, mem=500GB&nbsp;</td><td> &nbsp;cpu=128, mem=1.4TB  &nbsp;</td></tr><tr><td>Nodes &nbsp;Available&nbsp;</td><td>23 sml, 29 med, 3 lrg, 20 il  *</td><td>3 sml, 3 med, 3 il *&nbsp;</td><td>23 sml, 29 med, 3 lrg, 20 il *</td><td>3 med, 2 lrg, 1 cl *</td></tr><tr><td>Time limit  &nbsp;</td><td>2 days</td><td>  1 day</td><td> 14 days</td><td> &nbsp;2 days</td></tr></tbody></table></figure><p style="text-align:center;"><span class="fontSizeSmall">*<strong>il</strong> is the icelake node and <strong>cl</strong> is the cooperlake node, <strong>which are the newly purchased nodes</strong></span></p><p>Other changes include the following.</p><ol><li>Changes to the GPU node&nbsp;&nbsp;<ol><li>A100 GPUs will be available to everyone with no access request required.</li><li>The GPU partition will be&nbsp;available to Open OnDemand Jupyter notebooks.&nbsp;&nbsp;​​​​​​​</li></ol></li><li>Increase the memory limit of RStudio OnDemand&nbsp;to 500 GB on the regular partition&nbsp;with a time limit of 48 hours.&nbsp;</li></ol><p><span class="fontColorRedDark"><strong>The changes are to take place with the April outage (Thursday 06&nbsp;April to 08&nbsp;April 2023).</strong></span></p>
<h3><span class="fontColorThemeSecondary">New SLURM Partitions</span>&nbsp;<span class="fontColorRedDark"><strong>COMING SOON!</strong></span><br>&nbsp;</h3><ol><li>An AMD partition will be added for testing purposes, containing one node with AMD cores. Researchers will be allowed to use all the cores, but jobs will have a restriction on wall times; the maximum wall time will be 6 hours.&nbsp;&nbsp;</li><li>A datamover partition will be added for data transfer.&nbsp;</li></ol>
<h3><span class="fontColorThemeSecondary">Changes to the RStudio Servers</span></h3>
<p>Currently, we have two RStudio servers, each featuring 477GB and 56 cores, equivalent to a medium node on SLURM, with 61 active users. The existing user limit is half a server, approximately 234GB and 28 cores. Over the past six months, we encountered seven server restarts due to unresponsiveness. One restart occurred between October and December 2022, while six were necessary between January and March 2023. This increase can be attributed to memory pressure, which affects all users, not just those responsible for overloading the server.</p><p>To address these concerns and improve the fair distribution, availability, and uptime of the servers, the&nbsp;following will be implemented.</p><ul><li><strong>Reduce the user limit on the servers</strong> to approximately 1/4 of a server, i.e., around 115GB and 14 cores. This adjustment will prevent individual users from monopolizing resources, accommodate more users, and avert memory overloads. Users with higher memory requirements will be encouraged to use RStudio OnDemand.</li><li><strong>Increase the memory limit on RStudio OnDemand</strong> to 500GB (this update is already scheduled for the April outage).</li></ul><p>Training and support will be provided to help researchers move their workloads to OnDemand.</p>
<p>The new nodes are almost ready! See them getting hooked up to the rest of Milton:</p><div tabindex="-1" data-cke-widget-wrapper="1" data-cke-filter="off" class="cke_widget_wrapper cke_widget_block cke_widget_inlineimage cke_widget_wrapper_webPartInRteInlineImage cke_widget_wrapper_webPartInRteClear cke_widget_wrapper_webPartInRteAlignCenter cke_widget_wrapper_webPartInRte" data-cke-display-name="div" data-cke-widget-id="1" role="region" aria-label="Inline image in RTE. Use Alt + F11 to go to toolbar. Use Alt + P to open the property pane."><div data-webpart-id="image" class="webPartInRte webPartInRteAlignCenter webPartInRteClear webPartInRteInlineImage cke_widget_element" data-cke-widget-data="%7B%22classes%22%3A%7B%22webPartInRteInlineImage%22%3A1%2C%22webPartInRteClear%22%3A1%2C%22webPartInRteAlignCenter%22%3A1%2C%22webPartInRte%22%3A1%7D%7D" data-cke-widget-upcasted="1" data-cke-widget-keep-attr="0" data-widget="inlineimage" data-instance-id="db460fb4-42d1-4152-9c49-219c89bd47dd" title=""></div></div><p>The new nodes will be</p><p style="text-align:start;"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>• 20 x 96 Threads, 512GB RAM – Intel 'Ice Lake' - il-n[01-20].&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start;"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>TOTAL: 1920 threads, 10TB mem</b></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start;"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>• 1 x 192 Threads, 3TB RAM – Intel 'Cooper Lake' - cl-n01.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start;"><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>TOTAL: 192 threads, 3TB mem</b></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p style="text-align:start;">We will also be adding a singular AMD (Milan) node (em-n01) with 112 threads and 500GB RAM. This will be placed in a separate partition and will be available for researchers to test and evaluate. Researchers will be allowed to use all the cores, but jobs will have a restriction on wall times. Initial testing indicates existing software modules and Python/R packages should work without issue on this node.</p><p style="text-align:start;">Let RCP know at research.computing@wehi.edu.au if you would be interested in testing the node and require longer wall times.</p>
<h4><span class="fontColorThemeSecondary">Testing</span></h4><div><p>The tests were to ensure that existing software modules and your R/Python package should work without issue on the new nodes, as well as get an expectation of changes in run times with the new nodes.</p><p><strong><span><span>The below modules and tools were tested and we can happily confirm that your existing pipelines is likely to work without issue on the new nodes and packages shouldn't need to be reinstalled.</span></span></strong></p><p>As for performance: see the below table with the software modules tested and the raw times (in seconds). The Broadwell and Skylake columns are the existing sml/med, and lrg nodes respectively. The Icelake and Cooperlake columns are the new nodes, with Icelake succeeding Broadwell and Cooperlake succeeding Skylake.</p><p><strong>The headline stats: Icelake nodes are faster than existing Broadwell nodes by about 20% on average, and up to 37% faster (numpy). Cooperlake is faster than existing Skylake nodes by about 14% on average, and up to 27% (nf-core/atacseq pipeline).</strong></p><div class="canvasRteResponsiveTable"><div class="tableWrapper"><table style="width:563px;" class=" cke_show_border"><tbody><tr><td style="width:224px;"><span class="fontSizeMedium"><strong>Software module</strong></span></td><td style="width:39px;"><span class="fontSizeMedium"><strong>Broadwell (sml/med)</strong></span></td><td style="width:21px;"><p><span class="fontSizeMedium"><strong>Icelake</strong></span><br><strong>(il)</strong></p></td><td style="width:46px;"><p><span class="fontSizeMedium"><strong>Skylake (lrg)</strong></span></p></td><td style="width:25px;"><span class="fontSizeMedium"><strong>Cooperlake<br>(cl)</strong></span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">bwa/0.7.17 + gatk/4.2.5.0 SamSort<br>(4 cores, 1 thread per core)</span></td><td style="width:39px;"><span class="fontSizeMedium">2242</span></td><td style="width:21px;"><span class="fontSizeMedium">1891</span></td><td style="width:46px;"><span class="fontSizeMedium">2319.2</span></td><td style="width:25px;"><span class="fontSizeMedium">2098</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">bwa/0.7.17 + gatk/4.2.5.0 SamSort<br>(4 cores, 2 threads per core)</span></td><td style="width:39px;"><span class="fontSizeMedium">1692</span></td><td style="width:21px;"><span class="fontSizeMedium">1435</span></td><td style="width:46px;"><span class="fontSizeMedium">1799</span></td><td style="width:25px;"><span class="fontSizeMedium">1594</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">NAMD-multicore/2.14<br>(4 cores, 1 thread per core)</span></td><td style="width:39px;"><span class="fontSizeMedium">278</span></td><td style="width:21px;"><span class="fontSizeMedium">193</span></td><td style="width:46px;"><span class="fontSizeMedium">320</span></td><td style="width:25px;"><span class="fontSizeMedium">245</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">NAMD-multicore/2.14<br>(4 cores, 2 threads per core)</span></td><td style="width:39px;"><span class="fontSizeMedium">245</span></td><td style="width:21px;"><span class="fontSizeMedium">189</span></td><td style="width:46px;"><span class="fontSizeMedium">279</span></td><td style="width:25px;"><span class="fontSizeMedium">249</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">hmmer/3.3.2</span></td><td style="width:39px;"><span class="fontSizeMedium">132</span></td><td style="width:21px;"><span class="fontSizeMedium">132</span></td><td style="width:46px;"><span class="fontSizeMedium">166</span></td><td style="width:25px;"><span class="fontSizeMedium">149</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">bcftools/1.16</span></td><td style="width:39px;"><span class="fontSizeMedium">2510</span></td><td style="width:21px;"><span class="fontSizeMedium">1969</span></td><td style="width:46px;"><span class="fontSizeMedium">2644</span></td><td style="width:25px;"><span class="fontSizeMedium">2327</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">R/4.2.1</span></td><td style="width:39px;"><span class="fontSizeMedium">171</span></td><td style="width:21px;"><span class="fontSizeMedium">170</span></td><td style="width:46px;"><span class="fontSizeMedium">192</span></td><td style="width:25px;"><span class="fontSizeMedium">192</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">R/openBLAS/4.2.1</span></td><td style="width:39px;"><span class="fontSizeMedium">46</span></td><td style="width:21px;"><span class="fontSizeMedium">33</span></td><td style="width:46px;"><span class="fontSizeMedium">52</span></td><td style="width:25px;"><span class="fontSizeMedium">42</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/sarek</span></td><td style="width:39px;"><span class="fontSizeMedium">166.25</span></td><td style="width:21px;"><span class="fontSizeMedium">115.25</span></td><td style="width:46px;"><span class="fontSizeMedium">160.25</span></td><td style="width:25px;"><span class="fontSizeMedium">132.67</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/chipseq</span></td><td style="width:39px;"><span class="fontSizeMedium">1527</span></td><td style="width:21px;"><span class="fontSizeMedium">1085.25</span></td><td style="width:46px;"><span class="fontSizeMedium">1588.25</span></td><td style="width:25px;"><span class="fontSizeMedium">1338.25</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/atacseq</span></td><td style="width:39px;"><span class="fontSizeMedium">2175.25</span></td><td style="width:21px;"><span class="fontSizeMedium">1478.5</span></td><td style="width:46px;"><span class="fontSizeMedium">2480.25</span></td><td style="width:25px;"><span class="fontSizeMedium">1816.75</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/mag</span></td><td style="width:39px;"><span class="fontSizeMedium">1704.25</span></td><td style="width:21px;"><span class="fontSizeMedium">1426.75</span></td><td style="width:46px;"><span class="fontSizeMedium">2060.25</span></td><td style="width:25px;"><span class="fontSizeMedium">1613.5</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/methylseq</span></td><td style="width:39px;"><span class="fontSizeMedium">207.5</span></td><td style="width:21px;"><span class="fontSizeMedium">169.25</span></td><td style="width:46px;"><span class="fontSizeMedium">221.75</span></td><td style="width:25px;"><span class="fontSizeMedium">184.25</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/nanoseq</span></td><td style="width:39px;"><span class="fontSizeMedium">458</span></td><td style="width:21px;"><span class="fontSizeMedium">411.75</span></td><td style="width:46px;"><span class="fontSizeMedium">569.5</span></td><td style="width:25px;"><span class="fontSizeMedium">514.75</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">nextflow: nf-core/viralrecon</span></td><td style="width:39px;"><span class="fontSizeMedium">1475</span></td><td style="width:21px;"><span class="fontSizeMedium">1140</span></td><td style="width:46px;"><span class="fontSizeMedium">1473.75</span></td><td style="width:25px;"><span class="fontSizeMedium">1406.75</span></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium">python/3.8.8: numpy benchmark</span></td><td style="width:39px;"><span class="fontSizeMedium">12.8e-5</span></td><td style="width:21px;"><span class="fontSizeMedium">8.02e-5</span></td><td style="width:46px;"><span class="fontSizeMedium">11.0e-5</span></td><td style="width:25px;"><p><span class="fontSizeMedium">9.48e-5</span></p></td></tr><tr><td style="width:224px;"><span class="fontSizeMedium"><strong>Geometric mean</strong></span></td><td style="width:39px;"><span class="fontSizeMedium"><strong>207.03</strong></span></td><td style="width:21px;"><span class="fontSizeMedium"><strong>162.75</strong></span></td><td style="width:46px;"><span class="fontSizeMedium"><strong>225.06</strong></span></td><td style="width:25px;"><span class="fontSizeMedium"><strong>193.16</strong></span></td></tr></tbody></table></div></div><p><br></p></div>
<p>Earlier this year, RCP went through a series of engagement rounds with the Milton users community, to discuss changes to SLURM partitions and configuration. Details are available in the document below and also in <span class="fontColorBlue"><strong><a href="/sites/mug/SitePages/TrainingHome.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/mug/SitePages/TrainingHome.aspx" target="_blank" data-interception="off" title="https://wehieduau.sharepoint.com/sites/mug/SitePages/TrainingHome.aspx"><span class="fontColorBlue">MUG shared documents</span></a>.&nbsp;</strong></span>​​​​​​​</p><p><span><span><span>The main points that needed to be covered according to the discussions were as follows:<br>​​​​​​​</span></span></span></p><ol><li><span><span><span>Enhance availability of service for interactive jobs </span></span></span></li><li><span><span><span>Provide a mechanism for long-running jobs without time extension requests through the helpdesk</span></span></span></li><li><span><span><span>Provide a dedicated partition for large memory jobs and enhance availability of service</span></span></span></li><li><span><span><span>Ensure that more small users have access to the batch system</span></span></span></li><li><span><span><span>Provide a bonus (pre-emptive) queue which ensures no user is denied service if resources are idle.</span></span></span></li></ol>
<h3><span class="fontColorThemeSecondary">Current versus New SLURM Partitions</span></h3>
<p>Changes that will take place is highlighted in the table below.</p><div class="canvasRteResponsiveTable"><div class="tableLeftAlign tableWrapper"><table width="1145" class="filledHeaderTableStyleTheme cke_show_border"><colgroup><col><col><col><col><col><col><col><col><col><col><col><col><col><col><col></colgroup><tbody><tr><td role="columnheader" style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:2px solid black;vertical-align:middle;width:94px;"><p style="text-align:left;"><br></p></td><td colspan="2" role="columnheader" style="text-align:center;vertical-align:middle;width:148px;"><p style="text-align:center;"><span class="fontSizeSmall">reqular</span></p></td><td colspan="2" role="columnheader" style="text-align:center;vertical-align:middle;width:164px;"><p style="text-align:center;"><span class="fontSizeSmall">interactive</span></p></td><td colspan="2" role="columnheader" style="text-align:center;vertical-align:middle;width:279px;"><p style="text-align:center;"><span class="fontSizeSmall">long</span></p></td><td colspan="2" role="columnheader" style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:2px solid black;text-align:center;vertical-align:middle;width:127px;"><p style="text-align:center;"><span class="fontSizeSmall">gpu_interactive</span></p></td><td colspan="2" role="columnheader" style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:2px solid black;text-align:center;vertical-align:middle;width:138px;"><p style="text-align:center;"><span class="fontSizeSmall">bigmem</span></p></td></tr><tr><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:94px;"><p><br></p></td><td style="vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">Current</span></strong></span></p></td><td style="vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">New</span></strong></span></p></td><td style="vertical-align:bottom;width:82px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">Current</span></strong></span></p></td><td style="vertical-align:bottom;width:26px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">New</span></strong></span></p></td><td style="vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">Current</span></strong></span></p></td><td style="vertical-align:bottom;width:101px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">New</span></strong></span></p></td><td style="vertical-align:bottom;width:67px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">Current</span></strong></span></p></td><td style="vertical-align:bottom;width:69px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">New</span></strong></span></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">Current</span></strong></span></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:80px;"><p style="text-align:left;"><span class="fontSizeSmall"><strong><span class="fontColorBlue">New</span></strong></span></p></td></tr><tr><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:center;vertical-align:bottom;width:94px;"><p><strong><span class="fontColorBlue"><span class="fontSizeSmall">MaxSubmitPU&nbsp;</span></span></strong></p></td><td style="text-align:right;vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="fontSizeSmall">10000</span></p></td><td style="text-align:right;vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="highlightColorYellow"><span class="fontSizeSmall">5000</span></span></p></td><td style="text-align:right;vertical-align:bottom;width:82px;"><p style="text-align:left;"><span class="fontSizeSmall">1</span></p></td><td style="text-align:right;vertical-align:bottom;width:26px;"><p style="text-align:left;"><span class="fontSizeSmall">1</span></p></td><td style="text-align:right;vertical-align:bottom;width:51px;"><p style="text-align:left;"><span class="fontSizeSmall">2</span></p></td><td style="text-align:right;vertical-align:bottom;width:101px;"><p style="text-align:left;"><span class="highlightColorYellow"><span class="fontSizeSmall">3</span></span></p></td><td style="text-align:right;vertical-align:bottom;width:67px;"><p style="text-align:left;"><span class="fontSizeSmall">2</span></p></td><td style="vertical-align:bottom;width:69px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:right;vertical-align:bottom;width:80px;"><p style="text-align:left;"><span class="highlightColorYellow"><span class="fontSizeSmall">3</span></span></p></td></tr><tr><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:center;vertical-align:bottom;width:94px;"><p><strong><span class="fontColorBlue"><span class="fontSizeSmall">MaxJobPU&nbsp;</span></span></strong></p></td><td style="vertical-align:bottom;width:51px;"><p><br></p></td><td style="vertical-align:bottom;width:51px;"><p><br></p></td><td style="vertical-align:bottom;width:82px;"><p><br></p></td><td style="vertical-align:bottom;width:26px;"><p><br></p></td><td style="vertical-align:bottom;width:51px;"><p><br></p></td><td style="text-align:right;vertical-align:bottom;width:101px;"><p style="text-align:left;"><span class="highlightColorYellow"><span class="fontSizeSmall">1</span></span></p></td><td style="text-align:right;vertical-align:bottom;width:67px;"><p style="text-align:left;"><span class="fontSizeSmall">1</span></p></td><td style="vertical-align:bottom;width:69px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:right;vertical-align:bottom;width:80px;"><p style="text-align:left;"><span class="highlightColorYellow"><span class="fontSizeSmall">1</span></span></p></td></tr><tr><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:center;vertical-align:bottom;width:94px;"><p><strong><span class="fontColorBlue"><span class="fontSizeSmall">MaxTRESPU&nbsp;</span></span></strong></p></td><td style="vertical-align:middle;width:51px;"><p><span class="fontSizeSmall">cpu=994, mem=5952G&nbsp;</span></p></td><td style="vertical-align:middle;width:51px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">cpu=256, mem=1.3TB&nbsp;</span></span></p></td><td style="vertical-align:middle;width:82px;"><p><span class="fontSizeSmall">cpu=16, mem=64G&nbsp;</span></p></td><td style="vertical-align:middle;width:26px;"><p><span class="fontSizeSmall">cpu=16 mem=64G</span></p></td><td style="vertical-align:middle;width:51px;"><p><span class="fontSizeSmall">cpu=2, mem=8G</span></p></td><td style="vertical-align:middle;width:101px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">cpu=56, mem=350G&nbsp;</span></span></p></td><td style="vertical-align:middle;width:67px;"><p><span class="fontSizeSmall">cpu=12, gpu:a10=1, mem=62G, node=1&nbsp;</span></p></td><td style="vertical-align:middle;width:69px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:middle;width:80px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">cpu=128, mem=1.4TB&nbsp;</span></span></p></td></tr><tr><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:center;vertical-align:bottom;width:94px;"><p><strong><span class="fontColorBlue"><span class="fontSizeSmall">Nodes</span></span></strong></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="fontSizeSmall">24 sml, 30 med, 4 lrg&nbsp;</span></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">23 sml, 29 med, 3 lrg&nbsp;</span></span></p></td><td style="vertical-align:bottom;width:82px;"><p><span class="fontSizeSmall">2 sml, 2 med</span></p></td><td style="vertical-align:bottom;width:26px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">3 sml, 3 med</span></span></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="fontSizeSmall">1 sml, 1 med</span></p></td><td style="vertical-align:bottom;width:101px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">23 sml, 29 med</span></span></p></td><td style="vertical-align:bottom;width:67px;"><p><span class="fontSizeSmall">1 A10</span></p></td><td style="vertical-align:bottom;width:69px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p><br></p></td><td style="border-bottom:1px solid #7f7f7f;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:80px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">3 med, 2 lrg</span></span></p></td></tr><tr><td style="border-bottom:2px solid black;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;text-align:center;vertical-align:bottom;width:94px;"><p><strong><span class="fontColorBlue"><span class="fontSizeSmall">Time limit&nbsp;</span></span></strong></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="fontSizeSmall">2 days</span></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="fontSizeSmall">2 days</span></p></td><td style="vertical-align:bottom;width:82px;"><p><span class="fontSizeSmall">1 day&nbsp;</span></p></td><td style="vertical-align:bottom;width:26px;"><p><span class="fontSizeSmall">1 day</span></p></td><td style="vertical-align:bottom;width:51px;"><p><span class="fontSizeSmall">14 days</span></p></td><td style="vertical-align:bottom;width:101px;"><p><span class="fontSizeSmall">14 days</span></p></td><td style="vertical-align:bottom;width:67px;"><p><span class="fontSizeSmall">6 hrs</span></p></td><td style="vertical-align:bottom;width:69px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">10 hours</span></span></p></td><td style="border-bottom:2px solid black;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:58px;"><p><br></p></td><td style="border-bottom:2px solid black;border-left:2px solid black;border-right:2px solid black;border-top:1px solid #7f7f7f;vertical-align:bottom;width:80px;"><p><span class="highlightColorYellow"><span class="fontSizeSmall">2 days</span></span></p></td></tr></tbody></table></div></div>
<p>We decided to implement them in small steps, that will be tested for a week before the next step is implemented.&nbsp;The time plan to implement the changes is as follows:</p>
<ol><li><span class="fontColorBlue">A minor change to gpuq_interactive will occur, as per users'&nbsp;request to<strong> increase the time-limit</strong> to be a full-working day. This will avoid disruption of work.&nbsp;</span><span class="fontColorBlue">This may change if contention takes place.</span><br><br></li><li><span class="fontColorBlue">All changes should not cause any job loss or disruption to SLURM. However, if you face any issues, please email the </span><strong><a title="mailto:support@wehi.edu.au" data-interception="off" data-cke-saved-href="mailto:support@wehi.edu.au" href="mailto:support@wehi.edu.au" target="_blank">Helpdesk</a></strong><span class="fontColorBlue"><strong> </strong>with a detailed description of the issue you are facing.</span></li></ol>
<h3><span class="fontColorThemeSecondary">Move from virtualisation to bare-metal</span>​​​​​​​</h3>
<p>SLURM includes nodes of varying sizes as shown below. They are shared between the different SLURM partitions.&nbsp;<br></p><div class="canvasRteResponsiveTable"><div class="tableCenterAlign tableWrapper"><table style="width:625px;" class="filledHeaderTableStyleTheme cke_show_border" width="418"><colgroup><col><col></colgroup><tbody><tr><td role="columnheader" style="width:195px;">Node</td><td role="columnheader" style="width:92px;">Memory</td><td role="columnheader" style="width:70px;">CPU threads</td><td role="columnheader" style="width:166px;">Quantity</td></tr><tr><td style="width:195px;"><strong>milton-med-[001-026]</strong></td><td style="width:92px;">483G</td><td style="width:70px;">56</td><td style="width:166px;">26</td></tr><tr><td style="width:195px;"><strong>milton-sml-[001-024]</strong></td><td style="width:92px;">112G</td><td style="width:70px;">56</td><td style="width:166px;">24</td></tr><tr><td style="width:195px;"><strong>milton-lrg-[001-004]</strong></td><td style="width:92px;">1.4T</td><td style="width:70px;">128</td><td style="width:166px;">4</td></tr></tbody></table><p><br></p><p>After conversion to bare-metal and the addition of more nodes that were released from the VM pool, SLURM now has</p><p><br></p></div></div><div class="canvasRteResponsiveTable"><div class="tableCenterAlign tableWrapper"><table style="width:478px;" class="filledHeaderTableStyleTheme cke_show_border" width="418"><tbody><tr><td role="columnheader" style="width:159px;">Node</td><td role="columnheader" style="width:47px;">Memory</td><td role="columnheader" style="width:76px;">CPU threads</td><td role="columnheader" style="width:107px;">Quantity</td></tr><tr><td style="width:159px;"><strong>med-n[001-030]</strong></td><td style="width:47px;">501G</td><td style="width:76px;">56</td><td style="width:107px;">26</td></tr><tr><td style="width:159px;"><strong>sml-n[001-024]</strong></td><td style="width:47px;">123G</td><td style="width:76px;">56</td><td style="width:107px;">24</td></tr><tr><td style="width:159px;"><strong>lrg-n[001-004]</strong></td><td style="width:47px;">1.5T</td><td style="width:76px;">128</td><td style="width:107px;">4</td></tr></tbody></table></div></div><p><br></p>
