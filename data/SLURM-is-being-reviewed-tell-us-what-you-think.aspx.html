<h3><span class="fontColorRed">UPDATE - 5 April 2022</span></h3><h4><span class="fontColorRed">The SLURM review changes discussed in this article have been approved by Research Computing Steering Group (RCSG) and will be implemented soon.&nbsp;</span>​​​​​​​</h4>
<h2 style="text-align:left;">SLURM review - March-April 2022 - let us know what changes you would like made</h2><div><div><div><div><div><div><div><div><h3>​​​​​​​<span class="fontColorThemeSecondary">Time for a review</span></h3></div><p>Milton has been using SLURM for almost two years now, and it is time to evaluate and make changes, if needed, to ensure that it is fulfilling the majority of Milton users' needs.&nbsp;</p><p>RCP started a round of engagements with Milton users through emails on&nbsp;MUG (Milton User Group) and also through meetings and individual email threads which were collected into the&nbsp;<a title="https://wehieduau.sharepoint.com/:w:/s/mug/EYIBnm1yRclCvpQkAd-XIlMB5cXqhbSdbMuzNBpoUaxpQA?e=DRPIt4" data-interception="off" data-cke-saved-href="https://wehieduau.sharepoint.com/:w:/s/mug/EYIBnm1yRclCvpQkAd-XIlMB5cXqhbSdbMuzNBpoUaxpQA?e=DRPIt4" href="/:w:/s/mug/EYIBnm1yRclCvpQkAd-XIlMB5cXqhbSdbMuzNBpoUaxpQA?e=DRPIt4" target="_blank">SLURM Partition Review</a><strong>&nbsp;</strong>document. Please read this document to learn what is proposed so far, whenever you have time. (<span class="fontColorRed">This document was closed on 5 April 2022</span>)</p><div><h3><span class="fontColorThemeSecondary">Changes Proposed</span></h3></div><div><h4><span class="fontColorBlue">Interactive partition&nbsp;</span></h4></div><p>To ensure the availability of interactive service, one small and one medium node will be exclusively allocated to the <em>interactive</em> partition and the rest of the&nbsp;nodes will be shared with the <em>regular</em> partition.&nbsp;</p><div><h4><span class="fontColorBlue">Long partition</span></h4></div><p>This partition will cover the need for long-running jobs that can run for up to 14 days with moderate resources (cpu=56, mem=350G).&nbsp; The current <em>long</em>&nbsp;has restrictive resource limits that have affected its usability.</p><div><h4><span class="fontColorBlue">Big memory partition&nbsp;</span></h4></div><p>This is a new partition that will cover the need for large memory jobs up to 1.4TB. One large node will be exclusively allocated to the big memory partition to ensure availability for large jobs, in addition to shared nodes between <em>regular</em> and <em>long</em> partitions&nbsp;</p><div><h4><span class="fontColorBlue">Bonus Preemptive qos&nbsp;</span></h4></div><p>This is a newly added <em>qos</em> that is proposed to avoid idle resources. Users can run new jobs if the resources are available, even if their running jobs already hit the resources limit. Their jobs will be given lower priority and will be cancelled if a higher priority job is queued and waiting for resources.&nbsp;</p><p>It is the responsibility of the user to requeue/restart their jobs if they are cancelled. The minimum runtime before potentially canceling a job&nbsp;will be 0.&nbsp;</p><p>All nodes are available to be used with this qos, even the GPU nodes.</p></div></div></div></div></div><div><div><div><div><div><div><h4><br></h4></div><div><h4><span class="fontColorBlue">Change partition resource limit&nbsp;</span></h4></div><p>To ensure availability of service to the largest number of users during working hours, the resources limit will be modified on the <em>regular</em> queue to ~8% of the resources (cpu=256, mem=1.3TB) but with the other partition&nbsp;modifications proposed, a user can use overall up to <strong>~15%</strong> of the resources (cpu=454, mem=3TB) distributed over <em>regular</em>, <em>longq</em> and <em>bigmemq</em> partitions (excluding GPUs nodes). This is in addition to the ability to run on any of&nbsp;the partitions in preemptive mode if resources are available.&nbsp;&nbsp;</p><p>This may have an&nbsp;impact on users, as users will need to calculate the resources needed and choose a partition according to need. Training and documentation will be provided.&nbsp;</p><div><h4><span class="fontColorBlue">Maximum number of submitted jobs to the <em>regular</em> partition&nbsp;</span></h4></div><p>Recently ITS had issues with workflow managers that submit jobs to the queue at a rate that the SLURM controllers could not handle, that is why it is essential to lower this limit to avoid controllers crashing. Other HPC facilities use&nbsp;a lower limit than 10,000, for example, Pawsey uses 512 on most of its clusters.&nbsp;</p><div><h4><span class="fontColorBlue">Time-extension on the <em>regular</em> partition&nbsp;</span></h4></div><p>With the availability of the <em>longq</em>, requests for a time extension through the Helpdesk will be discouraged. If received, it will be dealt with on a case-by-case basis where the recommendation for a rerun on <em>longq</em> will be given, or an extension granted, however, this may cause a decrease in user priority. This will ensure that users will run long jobs in the long queue and avoid using the <em>regular</em> partition for long-running jobs. Extensions may be rejected if jobs are not using the allocated resources efficiently.&nbsp;</p><div><h3><span class="fontColorThemeSecondary">Feedback and comments</span></h3></div><p>Your feedback is very important to the review process, so please email your comments (short or long) to <a href="mailto:research.computing@wehi.edu.au" data-cke-saved-href="mailto:research.computing@wehi.edu.au" target="_blank" data-interception="off" title="mailto:research.computing@wehi.edu.au">research.computing@wehi.edu.au</a> and let us know what changes you think should be made so we can improve your SLURM experience.</p></div></div></div></div></div></div></div><p><br></p>
