<h2>Performance monitoring tools</h2><p>It is important to request appropriate resources from our batch jobs. Over specifying resources may mean that CPUs or memory your request can be idle, when someone else could have used them. It may also mean your job waits longer in the queue than it really needs to! But under specifying resources will result in excessive run times or cause jobs to fail, wasting your time and resources needed to re-run work. Correctly specifying resources for your batch jobs:<br></p><ul><li>helps all users by not holding up unused resources<br></li><li>reduces costs for WEHI</li><li>may result in your jobs being scheduled sooner.<br></li></ul><p>If you are developing a new workflow, it is quite likely that you will not know what resources your jobs need. Initial steps in determining resource requirements for software you are not familiar with include:</p><ul><li>Monitoring how much resources the workflow uses on your laptop on a smaller dataset<br></li><li>consulting the documentation<br></li><li>asking colleagues who have used the software</li><li>asking on the Milton slack channel</li><li>searching and posting on appropriate forums such as <a data-cke-saved-href="https://www.biostars.org/" href="https://www.biostars.org/" target="_blank" title="https://www.biostars.org/" data-interception="off">Biostars</a></li><li>approaching the developers</li><li>asking <a href="mailto:research.computing@wehi.edu.au" data-cke-saved-href="mailto:research.computing@wehi.edu.au" target="_blank" data-interception="off" title="mailto:research.computing@wehi.edu.au">Research Computing</a> if they know of other users in the Institute or have experience with the software.</li></ul><p>These steps are not guaranteed to provide an answer or even a rough guide, particularly for novel software. Furthermore, the resources required will depend on the input data. At some point, it will be necessary to test the resource requirements of the software against your data. Our general advice is as follows.<br></p><h3><span class="fontColorThemeSecondary">During initial pipeline development</span></h3><p>During the initial phase of your pipeline development, you should develop an understanding of the resources your tools require.<br><br></p><ol><li>If possible, create a small representative data set initially. In the case of fastq input, start with a subset of reads.</li><li>Specify modest resource requirements and scale up or down as required. Many (but not all) software tools will allow you to specify the resource requirements on the command.</li><li>For modest resources, you can test interactively in a shared resource such as <em><span class="fontColorBlue">vc7-shared</span></em> or <a data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Interactive-workloads.aspx" href="/sites/rc2/SitePages/Interactive-workloads.aspx" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Interactive-workloads.aspx" data-interception="on">SLURM interactive sessions</a>. For more on this subject, read the <a data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Interactive-workloads.aspx" href="/sites/rc2/SitePages/Interactive-workloads.aspx" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Interactive-workloads.aspx" data-interception="on">Interactive workloads</a> page.</li><li>For extra resource requirements submit to the <a data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx" href="/sites/rc2/SitePages/Getting-started-Slurm.aspx" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx" data-interception="on">SLURM batch system</a>.<br></li></ol><p>Usually, software you use will not make use of all the resources you have requested in your job. In many cases, you need to <em>manually</em> specify how many threads/CPUs to use and some programs may even ways to control memory usage! Consequently, make sure that the resources you request for your job have actually passed through to the program. In many cases, the software may fall back to an inappropriate default. For example, if you submit a 10 core job as:<br></p><pre><span class="fontColorRed">sbatch --nodes=1 --tasks-per-node=10 --mem=24G --time=00:10:00 job_script.sh</span>\n</pre><p>you need to make sure the software knows about it:</p><pre><span class="fontColorRed">samtools sort -@ 10 â€“m 24G &lt;other arguments&gt;</span>\n</pre><p>You can monitor your work as it runs with the <span class="fontColorBlue"><em>top</em></span> command on the node where you are running the job:<br></p><pre>$ <span class="fontColorRed">top -u $USER</span>\n</pre><p>The <em><span class="fontColorBlue">-u</span></em> option limits output to only your programs. The example output below shows that the "pindel" command is using almost 1100% of a CPU-core, which means it's effectively using 11 CPU cores in parallel.<br></p>
<p>You can experiment with resource parameters until you feel your job is running satisfactorily. Increasing resources, as long as the software is actually using them, will reduce run times but bear in mind that large resource requests may be hard to satisfy at times of contention, leading to increased queue times. If you need a faster turnaround, speak to <a href="mailto:research.computing@wehi.edu.au" data-cke-saved-href="mailto:research.computing@wehi.edu.au" target="_blank" data-interception="off" title="mailto:research.computing@wehi.edu.au">Research Computing</a>.</p><h3><span class="fontColorThemeSecondary">Scaling up</span></h3><p>As your pipeline matures you will want to do runs against full datasets. If these runs are large, you will want to avoid failures which in turn lead to wasted time (and resources). In this case, is it better to err on over-specifying resources? Analysis of runs, after the fact, will allow for fine-tuning of resource requests.<br></p><p>There are two main methods for reporting resources.</p><ul></ul>
<h4><span class="fontColorBlue">Method 1</span></h4><p>Use the SLURM <em><span class="fontColorBlue">--mail-type</span></em> and <em><span class="fontColorBlue">--mail-user</span></em> options to have the batch system email you information about job progress and job statistics. This can be specified on the <em><span class="fontColorBlue">sbatch</span></em> command as:</p><pre><span class="fontColorRed">sbatch --mail-type=END --mail-user &lt;you&gt;@wehi.edu.au &lt;more options&gt; &lt;your script&gt;</span>\n</pre><p>The <em><span class="fontColorBlue">--mail-user</span></em> option is required to specify the recipient email address (which should be you) and is required.<br>The <em><span class="fontColorBlue">--mail-type</span></em> option specifies the conditions under which email should be sent. In this case, when the job finishes or is aborted. The options can also be specified as directives in your job script.</p><pre>#SBATCH --mail-type=END\n#SBATCH --mail-user=&lt;you&gt;@wehi.edu.au\n</pre><p>You will receive an output that looks like this:</p><pre>Job ID: 438173\nCluster: milton\nUser/Group: thomas.e/bioinfadm\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:00:21 core-walltime Job Wall-clock time: 00:00:21 Memory Utilized: 736.00 KB Memory Efficiency: 0.07% of 1.00 GB\n</pre><p>In particular, you can use CPU and memory efficiencies to tune your resource request.</p>
<h4><span class="fontColorBlue">Method 2</span></h4><p>The second option is the <em><span class="fontColorBlue">seff </span></em>command.</p><pre>$ <span class="fontColorRed">seff 438173</span>\nJob ID: 438173\nCluster: milton\nUser/Group: thomas.e/bioinfadm\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:00:21 core-walltime\nJob Wall-clock time: 00:00:21\nMemory Utilized: 736.00 KB\nMemory Efficiency: 0.07% of 1.00 GB\n</pre><p>This is only useful for a single job.</p><p><br></p>
<h4><span class="fontColorThemeSecondary">Slurm Job GPU Utilization</span></h4><p>GPUs are a very expensive resource! So, when you're first starting out with using GPUs on your Slurm jobs, or if you're transitioning from using a single GPU to using multiple GPUs for a single job, you should inspect that your jobs are utilizing the requested GPU(s) correctly and effectively. If they don't your jobs may fail, take much longer, and/or hold up other jobs which may really need the GPU(s) your job is occupying! So like CPU jobs, when you're first starting out, you should test that your Slurm GPU jobs are working as expected with smaller problem sizes than you real workload.</p><p>Checking for correct GPU usage can be done in one of two ways: interactively with nvidia-smi, or inspecting the utilization after your Slurm job(s) have ended with NVIDIA DCGM.</p><h4>Interactively with nvidia-smi</h4><p>The simplest way to check whether your job is to ssh on the node you're running on and use the <em>nvidia-smi</em> and <em>watch</em> commands to inspect usage. For example:</p><pre>[yang.e@slurm-login02 ~]$ squeue -u $USER\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; JOBID PARTITION&nbsp;&nbsp;&nbsp;&nbsp; NAME&nbsp;&nbsp;&nbsp;&nbsp; USER ST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TIME&nbsp; NODES NODELIST(REASON)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8423989_9&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpuq&nbsp; 96n4A30&nbsp;&nbsp; yang.e&nbsp; R&nbsp;&nbsp;&nbsp; 1:34:55&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 gpu-a30-n01\n[yang.e@slurm-login02 ~]$ ssh gpu-a30-n01\nLast login: Wed Oct&nbsp; 5 11:49:21 2022 from milton-login02.hpc.wehi.edu.au\n[yang.e@gpu-a30-n01 yang.e]$ watch nvidia-smi\n\nEvery 2.0s: nvidia-smiWed Oct  5 11:50:37 2022\n\nWed Oct  5 11:50:37 2022\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A30          On   | 00000000:17:00.0 Off |                    0 |\n| N/A   35C    P0   169W / 165W |  22997MiB / 24258MiB |     56%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA A30          On   | 00000000:65:00.0 Off |                    0 |\n| N/A   31C    P0    78W / 165W |  20644MiB / 24258MiB |     99%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   2  NVIDIA A30          On   | 00000000:CA:00.0 Off |                    0 |\n| N/A   33C    P0   168W / 165W |  20644MiB / 24258MiB |     99%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n|   3  NVIDIA A30          On   | 00000000:E3:00.0 Off |                    0 |\n| N/A   33C    P0   164W / 165W |  20690MiB / 24258MiB |     99%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     18745      C   ...bin/guppy_basecall_server    22992MiB |\n|    1   N/A  N/A     18745      C   ...bin/guppy_basecall_server    20639MiB |\n|    2   N/A  N/A     18745      C   ...bin/guppy_basecall_server    20639MiB |\n|    3   N/A  N/A     18745      C   ...bin/guppy_basecall_server    20685MiB |\n+-----------------------------------------------------------------------------+</pre><ol><li>Here I start from the login node, and I already have a Slurm job with a GPU running.</li><li>I run the command: <em>squeue -u $USER </em>which gets the list of my running jobs.</li><li>My job is running on the <em>gpu-a30-n01</em>, so I ssh onto that node.</li><li>I then run the <em>watch nvidia-smi</em> command.</li></ol><p><em>nvidia-smi</em> prints a summary table of the current activity happening on the nodes' GPUs. <em>w</em><em>atch</em> refreshes this output every 2 seconds. From the nvidia-smi output, I can see that my job is correctly occupying all 4 A30 GPUs on this node.</p><p>Note that when there are multiple jobs running on the same node, you may have to match the PID or Process name (see bottom table) to your Slurm job with the <em>top</em>, <em>htop, </em>or <em>ps</em> command.</p><h4>Using the NVIDIA "dcgmstats" plugin</h4><p>The ITS team have integrated NVIDIA Data Centre GPU Monitor tool with Slurm to provide summary stats of GPU utilization for the Slurm job being submitted. To make use of the tool in your Slurm job, pass the option to srun, salloc, or sbatch:</p><pre>--comment=dcgmstats</pre><p>This option will produce the file</p><pre>dcgm-stats-&lt;JobID&gt;.out</pre><p>in the Slurm job's current working directory. Inside will be a table matching each GPU used in the job, and each table will have summary stats for each process than ran on that GPU. For example, for a job requesting one GPU and running 8 GPU tasks one after the other:</p><pre>$ cat dcgm-stats-8424223.out\nSuccessfully retrieved statistics for job: 8424223.\n+------------------------------------------------------------------------------+\n| GPU ID: 0                                                                    |\n+====================================+=========================================+\n|-----  Execution Stats  ------------+-----------------------------------------|\n| Start Time                         | Wed Oct  5 12:23:23 2022                |\n| End Time                           | Wed Oct  5 12:28:06 2022                |\n| Total Execution Time (sec)         | 283.06                                  |\n| No. of Processes                   | 8                                       |\n+-----  Performance Stats  ----------+-----------------------------------------+\n| Energy Consumed (Joules)           | 23410                                   |\n| Power Usage (Watts)                | Avg: 116.765, Max: 173.244, Min: 29.966 |\n| Max GPU Memory Used (bytes)        | 6525288448                              |\n| SM Clock (MHz)                     | Avg: 1368, Max: 1440, Min: 930          |\n| Memory Clock (MHz)                 | Avg: 1215, Max: 1215, Min: 1215         |\n| SM Utilization (%)                 | Avg: 52, Max: 100, Min: 0               |\n| Memory Utilization (%)             | Avg: 14, Max: 28, Min: 0                |\n| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |\n| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |\n+-----  Event Stats  ----------------+-----------------------------------------+\n| Single Bit ECC Errors              | 0                                       |\n| Double Bit ECC Errors              | 0                                       |\n| PCIe Replay Warnings               | 0                                       |\n| Critical XID Errors                | 0                                       |\n+-----  Slowdown Stats  -------------+-----------------------------------------+\n| Due to - Power (%)                 | 0                                       |\n|        - Thermal (%)               | 0                                       |\n|        - Reliability (%)           | Not Supported                           |\n|        - Board Limit (%)           | Not Supported                           |\n|        - Low Utilization (%)       | Not Supported                           |\n|        - Sync Boost (%)            | 0                                       |\n+--  Compute Process Utilization  ---+-----------------------------------------+\n| PID                                | 56022                                   |\n|     Avg SM Utilization (%)         | 10                                      |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 56256                                   |\n|     Avg SM Utilization (%)         | 8                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 56488                                   |\n|     Avg SM Utilization (%)         | 7                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 56738                                   |\n|     Avg SM Utilization (%)         | 5                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 56980                                   |\n|     Avg SM Utilization (%)         | 4                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 57221                                   |\n|     Avg SM Utilization (%)         | 9                                       |\n|     Avg Memory Utilization (%)     | 2                                       |\n| PID                                | 57469                                   |\n|     Avg SM Utilization (%)         | 6                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n| PID                                | 57702                                   |\n|     Avg SM Utilization (%)         | 6                                       |\n|     Avg Memory Utilization (%)     | 1                                       |\n+-----  Overall Health  -------------+-----------------------------------------+\n| Overall Health                     | Healthy                                 |\n+------------------------------------+-----------------------------------------+</pre><p>DCGM is also available as a command line tool on the GPU nodes which you can use while your job is running. Note that like <em>nvidia-smi</em>, you may need to use other process monitoring tools to match your running Slurm job(s) with GPU(s).</p><pre># while one a GPU node\ndcgmi -v -p &lt;your process ID&gt;</pre>
<p>A powerful tool to get access to detailed job history is <em><a href="https://slurm.schedmd.com/sacct.html" data-cke-saved-href="https://slurm.schedmd.com/sacct.html" target="_blank" title="https://slurm.schedmd.com/sacct.html" data-interception="off">sacct</a></em>. For example:</p><pre>$ <span class="fontColorRed">sacct --format JobID,Start,Elapsed,NCPUS,CPUTimeRaw,ReqMem,MaxRSS --user $USER --starttime 2020-09-01|head</span>\n       JobID               Start    Elapsed      NCPUS CPUTimeRAW     ReqMem     MaxRSS\n------------ ------------------- ---------- ---------- ---------- ---------- ----------\n323270       2020-08-31T16:59:59   11:59:28          1      43168        4Gn\n323270.batch 2020-08-31T16:59:59   11:59:28          1      43168        4Gn      2068K\n323270.exte+ 2020-08-31T16:59:59   11:59:28          1      43168        4Gn       516K\n323616       2020-09-02T11:05:33   00:00:01         20         20       32Gn\n323616.batch 2020-09-02T11:05:33   00:00:01         20         20       32Gn       817K\n323616.exte+ 2020-09-02T11:05:33   00:00:01         20         20       32Gn        28K\n323617       2020-09-02T11:10:51   00:01:10         20       1400       32Gn\n323617.batch 2020-09-02T11:10:51   00:01:11         20       1420       32Gn   2940871K\n</pre><p>Each job has three entries. For single-step jobs, just use the 'batch' entry.</p><p><em><span class="fontColorBlue">sacct</span></em> has many options for filtering, specifying output fields, and formatting to aid parsing for further analysis. See <em><span class="fontColorBlue">man sacct</span></em> for more details or go to <a href="https://slurm.schedmd.com/sacct.html" data-cke-saved-href="https://slurm.schedmd.com/sacct.html" target="_blank" title="https://slurm.schedmd.com/sacct.html" data-interception="off">slurm.schedmd.com/sacct</a>.</p><p>Most pipelines will have different resource requirements at different points in the processing. For example, in bioinformatics pipelines, the alignment step performs very well with large numbers of CPUs and modest memory, while some variant callers only use a single CPU.</p><p>You may achieve better throughput by using a pipeline tool that will:<br><br></p><ul><li>break the pipeline up into separate jobs each requesting the resources required for that step</li><li>run jobs in parallel when possible</li><li>restart the pipeline in the event of failure. Failure is inevitable.</li></ul>
