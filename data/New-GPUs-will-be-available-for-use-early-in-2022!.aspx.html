<h2>New GPUs are here &amp; will be available for use in early 2022<br></h2><p><span>We close this year with a nice present from ITS RS, the <b>new GPUs</b> are here and will be available for use <b>early 2022</b>!!&nbsp; <br></span></p><p><span><span>The G</span>PUs will be available from <b>the second week of January 2022</b>. At the moment, ITS RS team is still performing user acceptance testing after which the GPUs will be available to users.</span></p><p><span>We propose the configuration <a data-cke-saved-href="https://wehieduau.sharepoint.com/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" href="/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" target="_blank" title="https://wehieduau.sharepoint.com/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" data-interception="off">described here</a> for the SLURM queues. We will test this setup until the end of January 2022, while we gather and discuss your feedback, comments, and suggestions.&nbsp; Modification will be added to the <em>Slurm Queue Modification Report</em> and hopefully discussed/approved by the RCSG by the mid-end of February. Once you start using the queues, please leave your feedback in <a data-cke-saved-href="https://wehieduau.sharepoint.com/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" href="/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" target="_blank" title="https://wehieduau.sharepoint.com/sites/mug/SitePages/Milton-GPU-Queue-Design.aspx" data-interception="off">the Comments section</a>. <br></span></p><p><span>GPUs that will be available from the second week of 2022:</span><br></p><div class="canvasRteResponsiveTable"><div class="tableWrapper"><table title="Table" style="width:646px;" class="bandedRowColumnTableStyleTheme"><tbody><tr><td style="width:146px;" role="columnheader"><strong>GPU, GPU RAM<br></strong></td><td style="width:4px;text-align:center;" role="columnheader">Quantity<br></td><td style="width:121px;text-align:center;" role="columnheader">GPUs per node<br></td><td style="width:203px;" role="columnheader">Resources per node<br></td></tr><tr><td style="width:146px;" role="rowheader"><strong>A100, 40GB<br></strong></td><td style="width:4px;text-align:center;">6<br></td><td style="width:121px;text-align:center;">2<br></td><td style="width:203px;">96 threads, 1TB<br></td></tr><tr><td style="width:146px;" role="rowheader"><strong>A30, 24GB<br></strong></td><td style="width:4px;text-align:center;">28<br></td><td style="width:121px;text-align:center;">4<br></td><td style="width:203px;">96 threads, 512GB<br></td></tr><tr><td style="width:146px;" role="rowheader"><strong>A10, 24GB<br></strong></td><td style="width:4px;text-align:center;">4<br></td><td style="width:121px;text-align:center;">4<br></td><td style="width:203px;">48 threads, 256GB<br></td></tr><tr><td style="width:146px;" role="rowheader"><strong>P100, 12GB<br></strong></td><td style="width:4px;text-align:center;">20<br></td><td style="width:121px;text-align:center;">4<br></td><td style="width:203px;">48 threads, 108GB<br></td></tr><tr><td style="width:146px;" role="rowheader"><strong>M60, 8GB<br></strong></td><td style="width:4px;text-align:center;">2<br></td><td style="width:121px;text-align:center;">1<br></td><td style="width:203px;">25 threads, 128GB<br></td></tr></tbody></table></div></div><p><br></p>
