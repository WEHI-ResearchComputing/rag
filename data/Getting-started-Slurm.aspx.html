<h2>Getting started with SLURM</h2><blockquote><p>All WEHI academic staff/students will automatically have access to slurm.&nbsp;</p><p>If you face issues running jobs please&nbsp; email&nbsp;<a data-cke-saved-href="mailto:support@wehi.edu.au" href="mailto:support@wehi.edu.au" title="mailto:support@wehi.edu.au" data-interception="off"><span class="fontColorYellowLight">Helpdesk</span></a> with details of the issue.</p></blockquote>
<h2>Batch jobs: Getting started with SLURM</h2><p>On WEHI's Milton HPC, the&nbsp;<a data-interception="off" title="https://slurm.schedmd.com/" href="https://slurm.schedmd.com/" data-cke-saved-href="https://slurm.schedmd.com/">SLURM</a>&nbsp;(<strong>S</strong>imple&nbsp;<strong>L</strong>inux&nbsp;<strong>U</strong>tility for&nbsp;<strong>R</strong>esource&nbsp;<strong>M</strong>anagement) batch system is used.</p><p>Batch systems involve users submitting jobs to a scheduler and resource manager that decides the best and most efficient way to run the jobs while maintaining the highest possible usage of all resources.</p><p>The SLURM queuing system includes:&nbsp;<br><br></p><ul><li><strong>nodes</strong>&nbsp;</li><li>the&nbsp;<strong>compute</strong>&nbsp;resources in SLURM&nbsp;</li><li><strong>partitions</strong>&nbsp;which group nodes into logical sets</li><li><strong>jobs,</strong>&nbsp;or allocations of resources, assigned to a user for a specified amount of time; and</li><li><strong>job steps</strong>&nbsp;which are sets of (possibly parallel) tasks within a job.</li></ul><p>Batch systems usually have two types of nodes that users interact with:<br><br></p><ul><li><span class="fontColorBlue"><strong>submit nodes</strong></span>, also called login nodes, which are used for logging in, monitoring, and job submission</li><li>hundreds (or even thousands) of&nbsp;<span class="fontColorBlue"><strong>compute nodes</strong></span>, which execute jobs that users submit.</li></ul><p>Milton has the following login nodes:<br><br></p><ul><li><strong>vc7-shared.hpc.wehi.edu.au</strong></li><li><strong>slurm-login.hpc.wehi.edu.au</strong></li></ul><p>Please do not run workloads on slurm-login nodes. there are group limits applied to prevent that so any tasks may be killed. The group limits on the login nodes are as follows</p><ul><li><strong>vc7-shared:&nbsp;</strong>4 CPUs and 24GB</li><li><strong>slurm login:</strong> 2 CPUs and 8GB</li></ul><p>The rest of this guide describes how to start using SLURM, the main components of your submission script, and how to submit, monitor, and evaluate your job. It assumes basic familiarity with the <a data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Linux.aspx" href="/sites/rc2/SitePages/Linux.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Linux.aspx">Unix command line</a>. A list of external tutorials is available&nbsp;<a data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/External-training.aspx" href="/sites/rc2/SitePages/External-training.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/External-training.aspx">here</a>. For more details on creating a job submission script go to the <a data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx" href="/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx">SLURM advanced job scripts</a> guide, and to learn more about tuning your job go to the <a data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-job-tuning.aspx" href="/sites/rc2/SitePages/SLURM-job-tuning.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-job-tuning.aspx">SLURM job tuning</a> guide.</p>
<h4><span class="fontColorBlue">Contents on this page:</span></h4><p><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#batch-jobs-getting-started-with-slurm" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#batch-jobs-getting-started-with-slurm" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#batch-jobs-getting-started-with-slurm">Batch jobs</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#getting-started-with-slurm" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#getting-started-with-slurm" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#getting-started-with-slurm">Getting started with SLURM</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#logging-in-to-slurm-login-nodes" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#logging-in-to-slurm-login-nodes" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#logging-in-to-slurm-login-nodes">Logging in to SLURM login nodes</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#job-submission" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#job-submission" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#job-submission">Job submission</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#1%29-prepare-a-job-submission-script" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#1)-prepare-a-job-submission-script" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#1)-prepare-a-job-submission-script">1) prepare a job submission script</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-single-core-job" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-single-core-job" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-single-core-job">Example: single-core job</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-multi-core-job" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-multi-core-job" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#example-multi-core-job">Example: multi-core job</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#2%29-submit-your-job" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#2)-submit-your-job" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#2)-submit-your-job">2) submit your job</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#3%29-monitor-your-job" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#3)-monitor-your-job" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#3)-monitor-your-job">3) monitor your job</a><br><a href="/sites/rc2/SitePages/Getting-started-Slurm.aspx#4%29-evaluate-your-job-and-retrieve-results" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#4)-evaluate-your-job-and-retrieve-results" data-interception="on" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Getting-started-Slurm.aspx#4)-evaluate-your-job-and-retrieve-results">4) evaluate your job and retrieve results</a></p>
<h3><span class="fontColorThemeSecondary">Logging in to SLURM login nodes</span></h3><p>There are two <strong>SLURM</strong> login nodes called <strong><span class="fontColorBlue">slurm-login01</span></strong> and <strong><span class="fontColorBlue">slurm-login02</span></strong>. However, there is no need to login directly to either node, unless you have some applications running for pipeline management on that specific node. Both nodes have the same specifications. Usually, you will just need to ssh to slurm-login and the system will redirect you to one of the nodes.</p><p>For Linux, macOS, and Windows (MobaXterm), open a local terminal window (eg. Terminal, iTerm2, xTerm, etc) and type:</p><pre><span class="fontColorRed"><span class="fontSizeMedium">ssh slurm-login</span></span>\n</pre><p>Or to login to a specific node:</p><pre><span class="fontColorRed">ssh slurm-login01</span>\n</pre><p>The system will then ask you for your password. Type it in. You will see something similar to this on your screen:</p><pre>mac2962:~ <span class="fontColorRed">james.s$ ssh slurm-login</span>\njames.s@slurm-login's password:\nLast login: Thu Mar 26 09:45:55 2020 from mac2962.wehi.edu.au\n[james.s@slurm-login02 ~]$\n\n</pre><h3><span class="fontColorThemeSecondary">Job submission</span></h3><p>The steps for submitting jobs are:<br><br></p><ol><li>Prepare a job submission script</li><li>Submit your job</li><li>Monitor your job</li><li>Evaluate your job and retrieve the results.</li></ol>
<h4><span class="fontColorBlue">1) Prepare a job submission script</span></h4><p>To process a job using SLURM, a job submission script has to be created that states what work needs to be done and what resources should be allocated to the job. This script will then be run by submitting it to the SLURM queuing system using the <span class="fontColorBlue"><em>sbatch</em></span> command. This job waits in the queue with everyone else's job until the requested resources become available.</p><p>The script should state:</p><ul><li><span class="fontColorBlue"><em><strong>time</strong></em></span>: the maximum time for the job execution. <em>default = 2 days</em></li><li><em><span class="fontColorBlue"><strong>cpus-per-task</strong></span></em>: number of CPUs per task, such that the number of requested processing units per node is (ntasks*cpus-per-task). <em>default = 1</em></li><li><em><span class="fontColorBlue"><strong>nodes</strong></span></em>: the number of nodes used. <em>default = 1</em></li><li><em><span class="fontColorBlue"><strong>partition</strong></span></em>: the partition (queue) in which your job is placed. <em>default = regular</em></li><li><em><span class="fontColorBlue"><strong>mem</strong></span></em>, <span class="fontColorBlue"><em><strong>mem-per-cpu</strong></em></span>: the amount of physical (not virtual) memory. <em>default = 10MB</em></li><li><span class="fontColorBlue"><em><strong>gres</strong></em></span>: special resources such as GPUs or nodes with specific CPU types. <em>default = 0</em></li></ul><p>If one of the resource options is not specified, the default value is used.</p><p>For a <strong>single node, single task</strong> job (which is the most common job), we recommend you specify at least the following resources:</p><div class="canvasRteResponsiveTable"><div class="tableWrapper"><div class="canvasRteResponsiveTable"><div class="tableWrapper"><table class="filledHeaderTableStyleTheme cke_show_border"><tbody><tr><td role="columnheader" style="width:168px;">Resource</td><td role="columnheader" style="width:360px;"><strong><span class="fontColorYellowLight">#SBATCH</span> directive</strong></td><td role="columnheader" style="width:432px;"><strong>Description</strong></td></tr><tr><td style="width:168px;"><span class="fontSizeMediumPlus"><strong>1 node</strong></span></td><td style="width:360px;"><pre><span class="fontSizeMediumPlus">--Nodes=1</span></pre></td><td style="width:432px;"><span class="fontSizeMediumPlus">1 node required for your job</span></td></tr><tr><td style="width:168px;"><span class="fontSizeMediumPlus"><strong>Memory</strong></span></td><td style="width:360px;"><pre><span class="fontSizeMediumPlus">--mem=&lt;limit&gt;  \n<strong>eg.</strong> --mem=2G</span></pre></td><td style="width:432px;"><span class="fontSizeMediumPlus">Memory limit eg. 2GB</span></td></tr><tr><td style="width:168px;"><span class="fontSizeMediumPlus"><strong>Time limit</strong></span></td><td style="width:360px;"><pre><span class="fontSizeMediumPlus">--time=d-hh:mm:ss\n<strong>eg.</strong> --time=2:00:00</span></pre></td><td style="width:432px;"><p><br></p></td></tr></tbody></table></div></div></div></div><div class="canvasRteResponsiveTable"><div class="canvasRteResponsiveTable"><p>This will allocate, 1 node, 1 CPU, 2GB of memory for 2 hours. If no resources are specified in the job submit script, SLURM will default to:</p><ul><li>10 MB of memory</li><li>1 x CPU core</li><li>The time limit will vary between partitions.<br><br></li></ul><p>After specifying the SBATCH options, the script will then execute your commands, such as:<br><br></p><ul><li>load modules and configure environment&nbsp;(read more about the module system <a href="/sites/rc2/SitePages/modules.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Software.aspx" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Software.aspx" data-interception="on">here</a>)</li><li>change directory</li><li>run programs.</li></ul></div></div><div class="canvasRteResponsiveTable"><div class="tableCenterAlign tableWrapper"><div class="canvasRteResponsiveTable"><br></div><div class="canvasRteResponsiveTable"><strong><span class="fontColorRed">Note:</span></strong> <strong>Requesting the minimum amount of resources and time can help you job start sooner! </strong>Slurm will try to first push through the "highest priority" job, but in trying to satisfy all the different requests, there will be "gaps" in the schedule. Slurm will then try to find the next highest priority job that is small enough (in time or requested resources), and start that job to fill the gap! <strong>The default time 2 days may not be needed, and can make your job wait longer in the queue!</strong></div></div></div>
<h3><span class="fontColorThemeSecondary">Hyperthreading and SLURM</span></h3><p>Hyperthreading is a computing concept designed to better utilize CPUs for certain kinds of work. It assigns multiple "threads" (usually two) to a single physical core on the CPU. Each of these threads can run a list of instructions. Some of those instructions may require some wait time, for example when reading/writing from/to disk is occurring. So, while one thread is waiting, the physical CPU can switch threads and run instructions from another thread. This doesn't make the CPU <em>fast</em><em>er </em>per se, but instead it makes it <em>better utilized</em>, so more things can get done.</p><p>All our nodes are configured with hyperthreading, where each physical core has 2 hyper threads. Slurm is configured to interpret one thread as a CPU core. You control how many threads are requested using the sbatch options</p><pre>--ntasks=24\n--cpus-per-task=2\n</pre><p>which requests 24 x 2 = 48 hyper threads. 2 hyper threads will be assigned to each physical core. Note that if you request an odd number of hyper threads, your request will be rounded up to the nearest number as hyper threads on a single physical core cannot be used by multiple jobs (i.e., all-or-nothing). For example, the request</p><pre>salloc --ntasks=1 --cpus-per-task=5 --mem=2G</pre><p>will be allocated 6 threads not 5.</p><p>It is also good practice to set ntasks explicitly to avoid SLURM from assuming the number of tasks from the requested number of CPUs. The default is one task per node and one CPU per node but&nbsp;--<strong>ntasks</strong>&nbsp;and --<strong>cpus-per-task</strong>&nbsp;option can change this default.</p><p>For more details, go to <a title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Hyperthreading.aspx" data-interception="on" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/Hyperthreading.aspx" href="/sites/rc2/SitePages/Hyperthreading.aspx">this page.</a></p><h4>How do I make use of hyperthreading?</h4><p>Hyperthreading will automatically work when you start more processes than your physical core allocation. However, benefitting from hyperthreading is not guaranteed. Some types of parallel tasks that can benefit from hyperthreading include where each process: reads and/or writes from/to files frequently or operates on data in disparate places in memory.</p><p>Computation that likely won't benefit from hyperthreading:</p><ul><li>where parallel tasks require a high level of coordination (more tasks = more coordination),</li><li>compute intensive and not much I/O work (e.g. large matrix operations), or</li><li>where parallel tasks make frequent writes to a single file which is not in a format designed for parallel accesses e.g., SQLite.</li></ul><p>To not use hyperthreading, you must ensure that you do not execute more parallel processes than the number of physical cores requested, where</p><pre>physical cores = (ntasks x cpus-per-task)/2</pre><p>We strongly encourage you test whether or not your word would benefit from hyperthreading by running in test cases. In some cases, jobs could be sped up significantly while taking advantage of hyperthreading. But in other cases, can be slowed down substantially.</p>
<blockquote><p><span class="fontSizeLarge">SLURM will not run a job if there are not enough resources to allocate.<br>Sometimes you may need to test run your script a few times to determine how many resources are required to complete your job.<br>Read the <a title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-job-tuning.aspx" data-interception="on" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-job-tuning.aspx" href="/sites/rc2/SitePages/SLURM-job-tuning.aspx"><span class="fontColorYellowLight">Performance Monitoring Tools</span></a> guide for more help and information about job tuning.</span></p></blockquote>
<h3><span class="fontColorBlue">Example: single core job</span></h3><p>A single core job <strong><span class="fontColorThemeSecondary">singlecore.sh</span></strong>, which requests 1 node, 2 hyper threads (1 physical CPU core), and 8GB of RAM for 1 hour.</p><pre>#!/bin/bash\n#SBATCH --job-name=singlecore_job \n#SBATCH --time=01:00:00\n#SBATCH --ntasks=1\n#SBATCH --mem=8G\nmodule load python/3.8.3\npython mysinglecore.py\n</pre>
<h3><span class="fontColorBlue">Example: multi-core job</span></h3><p>A multi-core job script <strong><span class="fontColorThemeSecondary">multicore.sh</span></strong>, which requests 1 node, 8 hyper threads (4 physical CPU cores), and 8GB of RAM for 10 hours.</p><pre>#!/bin/bash\n#SBATCH --job-name=mcore_job\n#SBATCH --time=10:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=8G\n​​​​​​​#SBATCH --ntasks=1</pre><pre>module load python/3.8.3\npython mymulticore.py\n</pre>
<h3><span class="fontColorBlue">Example: multi-node job with MPI</span></h3><p>For some programs like Relion, parallelization on multiple nodes can be facilitated with MPI. A multi-node job script <strong><span class="fontColorThemeSecondary">multinode.sh</span></strong>, which requests 2 nodes, 8 hyper threads (4 physical CPU cores) on each node, and 8GB of RAM on each node for 10 hours.</p><pre>#!/bin/bash\n#SBATCH --job-name=mnode_job\n#SBATCH --time=10:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1G\n​​​​​​​#SBATCH --ntasks-per-node=8\n#SBATCH --nodes=1\n</pre><pre>module load openmpi/4.1.4\nmpirun -n ${SLURM_NTASKS} XXXcommandXXX</pre>
<div class="canvasRteResponsiveTable"><div class="tableWrapper"><table style="width:1127px;" class=" cke_show_border"><tbody><tr><td style="width:1099px;"><p><strong><span class="fontColorRed">Note:</span></strong></p><ul><li>When requesting GPUs, use the following sbatch directive, <em><span class="fontColorBlue">--gres=gpu:N</span></em> for <em><span class="fontColorBlue">N</span></em> GPUs. Milton has several nodes with 4 GPUs each. It is recommended to specify the partition gpuq: <em><span class="fontColorBlue">--partition=gpuq</span></em></li><li>We have 2 generations of Intel CPUs, Broadwell, and Skylake. If you want to execute on a specific CPU type you can use the constraint directive: ​​​​​​​<em><span class="fontColorBlue">--constraint=Skylake</span></em></li><li>Job scripts can be written either directly from the command line using nano or from programs such as Xcode, Sublime Text, TextEdit, etc.</li><li>Your job script can request for notifications to be sent out informing you (or others) of the progress of a job:<ul><li>--mail-type specifies which notifications to send. Common values for &lt;type(s)&gt; are BEGIN, END, FAIL, or ALL e.g. <span class="fontColorBlue"><em>--mail-type=BEGIN,END,FAIL</em></span> or <span class="fontColorBlue"><em>--mail-type=ALL</em></span></li><li>--mail-user specifies which address(es) to send the notification email. Multiple emails can be listed with a comma e.g. <span class="fontColorBlue"><em>--mail-user=abc@gmail.com,123@wehi.edu.au</em></span></li></ul></li></ul></td></tr></tbody></table></div></div><ul></ul>
<h4><span class="fontColorBlue">2) Submit your job</span></h4><p>Use the <a href="https://slurm.schedmd.com/sbatch.html" data-cke-saved-href="https://slurm.schedmd.com/sbatch.html" target="_blank" data-interception="off" title="https://slurm.schedmd.com/sbatch.html"><em><span class="fontColorBlue">s</span></em></a><em><a href="https://slurm.schedmd.com/sbatch.html" data-cke-saved-href="https://slurm.schedmd.com/sbatch.html" target="_blank" data-interception="off" title="https://slurm.schedmd.com/sbatch.html"><span class="fontColorBlue">batch</span></a></em> command, <em><span class="fontColorBlue">sbatch &lt;script&gt;</span></em>.</p><pre>$ <span class="fontColorRed">sbatch multicore.sh </span>\nSubmitted batch job 4365\n</pre><p>4365 is the job id.</p>
<h4><span class="fontColorBlue">&nbsp;3) Monitor your job</span></h4><p>There are multiple ways to monitor the progress of your job.</p><ul><li><em><strong><span class="fontColorBlue"><a data-cke-saved-href="https://slurm.schedmd.com/squeue.html" href="https://slurm.schedmd.com/squeue.html" target="_blank" title="https://slurm.schedmd.com/squeue.html" data-interception="off">squeue</a></span></strong></em></li><li><em><strong><span class="fontColorBlue">scontrol show job</span></strong></em></li><li><strong><a href="https://slurm.schedmd.com/sstat.html" data-cke-saved-href="https://slurm.schedmd.com/sstat.html" data-interception="on" title="https://slurm.schedmd.com/sstat.html"><em>sstat</em></a></strong></li></ul><p>To see the status of all the jobs in the SLURM queue, use <em><span class="fontColorBlue">squeue</span></em>:</p><pre>$ <span class="fontColorRed">squeue</span>\n        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        1286533   regular script.s ghangas.  R 1-00:00:14      1 milton-sml-014\n        1286532   regular script.s ghangas.  R 1-00:03:19      1 milton-sml-014\n        1287886   regular SRR10690 salleh.l  R    5:28:44      1 milton-sml-003\n        1287885   regular SRR10690 salleh.l  R    5:28:48      1 milton-sml-003          \n        1287884   regular SRR10690 salleh.l  R    5:28:51      1 milton-sml-002\n        1287883   regular SRR10690 salleh.l  R    5:28:55      1 milton-sml-002\n        1288100   regular nf-align bennett.  R    2:23:43      1 milton-sml-003\n        1288099   regular nf-align bennett.  R    2:23:46      1 milton-sml-001\n        1288098   regular nf-align bennett.  R    2:23:55      1 milton-sml-014\n        1284641   regular     wrap baldoni.  R 1-20:16:24      1 milton-sml-016\n        1284644   regular     wrap baldoni.  R 1-20:16:24      1 milton-sml-016\n        1284647   regular     wrap baldoni.  R 1-20:16:24      1 milton-sml-016\n</pre><p>To only see your jobs, you can use the <strong><em><span class="fontColorBlue">-u</span></em></strong> option:</p><pre>$ <span class="fontColorRed">squeue -u cryosparc</span>\n        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        1288143      gpuq cryospar cryospar  R    1:08:45      1 milton-gpu-001\n</pre><p>To see the state of a certain job, use <strong><em><span class="fontColorBlue">-j</span></em></strong></p><pre>$ <span class="fontColorRed">squeue -j 1288143</span>\n        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        1288143      gpuq cryospar cryospar  R    1:08:45      1 milton-gpu-001\n</pre><p>To show the expected start time of a pending job:</p><pre>$ <span class="fontColorRed">squeue -j &lt;jobID&gt; --start</span>\n</pre><p>To see detailed stats about the configuration of a <u>running or pending</u> job, use <em><span class="fontColorBlue">scontrol show job</span></em>:</p><pre>$ <span class="fontColorRed">scontrol show job 10892970</span>\nJobId=10892970 JobName=chute-cr0.1-rf0.1-notanghist\n&nbsp;&nbsp; UserId=yang.e(5087) GroupId=allstaff(10908) MCS_label=N/A\n&nbsp;&nbsp; Priority=104 Nice=0 Account=wehi QOS=normal\n&nbsp;&nbsp; JobState=RUNNING Reason=None Dependency=(null)\n&nbsp;&nbsp; Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n&nbsp;&nbsp; RunTime=22:42:50 TimeLimit=2-00:00:00 TimeMin=N/A\n&nbsp;&nbsp; SubmitTime=2023-03-16T10:57:01 EligibleTime=2023-03-16T10:57:01\n&nbsp;&nbsp; AccrueTime=2023-03-16T10:57:01\n&nbsp;&nbsp; StartTime=2023-03-16T10:57:19 EndTime=2023-03-18T10:57:19 Deadline=N/A\n&nbsp;&nbsp; PreemptEligibleTime=2023-03-16T10:57:19 PreemptTime=None\n&nbsp;&nbsp; SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-16T10:57:19 Scheduler=Backfill\n&nbsp;&nbsp; Partition=regular AllocNode:Sid=slurm-login:26387\n&nbsp;&nbsp; ReqNodeList=(null) ExcNodeList=(null)\n&nbsp;&nbsp; NodeList=sml-n21\n&nbsp;&nbsp; BatchHost=sml-n21\n&nbsp;&nbsp; NumNodes=1 NumCPUs=56 NumTasks=28 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n&nbsp;&nbsp; TRES=cpu=56,mem=11200M,node=1,billing=77\n&nbsp;&nbsp; Socks/Node=* NtasksPerN:B:S:C=28:0:*:* CoreSpec=*\n&nbsp;&nbsp; MinCPUsNode=56 MinMemoryCPU=200M MinTmpDiskNode=0\n&nbsp;&nbsp; Features=(null) DelayBoot=00:00:00\n&nbsp;&nbsp; OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n&nbsp;&nbsp; Command=/vast/scratch/users/yang.e/liggghts-flume/run-chute.sh\n&nbsp;&nbsp; WorkDir=/vast/scratch/users/yang.e/liggghts-flume\n&nbsp;&nbsp; StdErr=/vast/scratch/users/yang.e/liggghts-flume/logs/chute-cr0.1-rf0.1-notanghist-10892970.e\n&nbsp;&nbsp; StdIn=/dev/null\n&nbsp;&nbsp; StdOut=/vast/scratch/users/yang.e/liggghts-flume/logs/chute-cr0.1-rf0.1-notanghist-10892970.o\n&nbsp;&nbsp; Power=\n</pre><p>scontrol show job can be useful to obtain information about estimated start time for pending jobs, working directory and output and error writing location, and resource allocation.</p><p>To see detailed performance statistics like memory usage and disk write of a <u>running</u> job, you can use <span class="fontColorBlue"><em>sstat</em></span>:</p><pre>$ <span class="fontColorRed">sstat --format=jobid%15,avecpu%15,maxrss,maxdiskread,maxdiskwrite -j 10892970 -a</span>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; JobID&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AveCPU&nbsp;&nbsp;&nbsp;&nbsp; MaxRSS&nbsp; MaxDiskRead MaxDiskWrite\n--------------- --------------- ---------- ------------ ------------\n10892970.extern&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 00:00:00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 133K&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2012&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.00M\n&nbsp;10892970.batch&nbsp;&nbsp;&nbsp;&nbsp; 26-14:52:37&nbsp;&nbsp; 4417223K&nbsp;&nbsp;&nbsp; 772538297&nbsp;&nbsp; 7098940933</pre><p>Without the <span class="fontColorBlue"><em><strong>--format=...</strong></em></span> option, sstat will dump lots of information about the job. The above being a few that you might find useful. See which fields can be used in the --format option on <a href="https://slurm.schedmd.com/sstat.html#SECTION_Job-Status-Fields" data-cke-saved-href="https://slurm.schedmd.com/sstat.html#SECTION_Job-Status-Fields" data-interception="on" title="https://slurm.schedmd.com/sstat.html#SECTION_Job-Status-Fields">this page</a>. The <strong><em><span class="fontColorBlue">-a</span></em></strong> option asks sstat to return statistics about all "steps" of a job. Typically, the most relevant step is &lt;jobid&gt;.batch. <span class="fontColorBlue"><em><strong>-j</strong></em></span> is used to specify the JobID of the job you're interested in. Note that the command only works with your jobs i.e., you cannot query other people's jobs.<br></p><p><strong>Remember that how long your job waits is a function of how busy the queue is, how big your job is (generally smaller and less time, the better), and how much compute time you've used recently.</strong><br></p>
<div class="canvasRteResponsiveTable"><div class="tableWrapper"><table style="width:1098px;" class="simpleTableStyleNeutral cke_show_border"><tbody><tr><td><strong><span class="fontColorRed">&nbsp;&nbsp;&nbsp;&nbsp; TIP</span></strong></td><td style="text-align:center;width:969px;"><span class="fontSizeXLarge">To create a shortcut (alias) for <em><span class="fontColorBlue">squeue -u &lt;username&gt;</span></em>, use the alias command as follows:<br><em><span class="fontColorBlue">alias squ="squeue -u $USER"</span></em><br>and append it to your .bashrc.</span></td></tr></tbody></table></div></div>
<p>For all squeue options, go to <a href="https://slurm.schedmd.com/squeue.html" data-cke-saved-href="https://slurm.schedmd.com/squeue.html" target="_blank" title="https://slurm.schedmd.com/squeue.html" data-interception="off">squeue documentation.</a></p><p>To cancel a job i.e. remove it from the queue, use:</p><pre><span class="fontColorRed">scancel &lt;jobid&gt;</span>\n</pre><p>To see all of the scancel options, go to <a href="https://slurm.schedmd.com/scancel.html" data-cke-saved-href="https://slurm.schedmd.com/scancel.html" target="_blank" title="https://slurm.schedmd.com/scancel.html" data-interception="off">scancel documentation.</a></p><p>To see the <em>stdout</em> and <em>stderr</em> of your job, they are redirected by default to a file named <em><span class="fontColorBlue">slurm-&lt;jobid&gt;.out</span></em>. The file is saved by default in the directory where you executed the <em><span class="fontColorBlue">sbatch </span></em>command from.</p><p>To display its content:</p><pre><span class="fontColorRed">cat slurm-&lt;jobid&gt;.out</span>\n</pre><p>Or</p><pre><span class="fontColorRed">tail -f slurm-&lt;jobid&gt;.out</span>\n</pre><p>The filename can be changed by setting the SBATCH directives, <em><span class="fontColorBlue">--output &lt;filename&gt;</span></em> and <span class="fontColorBlue"><em>--error &lt;filename&gt;</em></span>. Go to our <a href="/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx" data-cke-saved-href="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx" title="https://wehieduau.sharepoint.com/sites/rc2/SitePages/SLURM-advanced-job-scripts.aspx" data-interception="on">Creating Job Submission Script guide</a> for more details.</p><p>Occasionally your job could fail without generating an error message, or providing an error message that is unclear, if this occurs raise a <a data-interception="off" title="mailto:support@wehi.edu.au" href="mailto:support@wehi.edu.au" data-cke-saved-href="mailto:support@wehi.edu.au" target="_blank">helpdesk</a> ticket and provide details about your job, for example Job ID, time of running, and the .out file if possible.</p>
<h4><span class="fontColorBlue">4) Evaluate your job and retrieve results</span></h4><p>Once your SLURM job has been completed, it is good practice to take a look at statistics related to the efficiency of resource use. This can be done through <em><span class="fontColorBlue">seff &lt;job_id&gt;</span></em>. For example,</p><pre>$ <span class="fontColorRed">seff 7959288</span>\n\nJob ID: 1025888\nCluster: milton\nUser/Group: &lt;user_id&gt;/allstaff\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 01:10:54\nCPU Efficiency: 97.73% of 01:12:33 core-walltime\nJob Wall-clock time: 01:12:33\nMemory Utilized: 3.63 MB\nMemory Efficiency: 36.26% of 10.00 MB\n</pre><p>From the above output, the CPU efficiency of the job (97%) shows the percentage of used CPU time in comparison to total CPU time, based on elapsed job time. Generally, you should aim for 100% utilization. However, it may be preferable not to occupy all CPUs (see the section above about hyperthreading), and so a reported 50% utilization would be ideal in these scenarios.</p><p>There is also Memory Efficiency, which you would aim to have 100% utilization. Anything less, and you may have over-allocated resources, i.e. requested resources (CPUs or memory) more than your work requires. In this situation, you are advised to reduce the requested resources the next time you run that job. Excessive over-allocation of resources takes away resources that could be used by others.</p><div class="canvasRteResponsiveTable"><div class="tableCenterAlign tableWrapper"><table style="width:873px;" class=" cke_show_border"><tbody><tr><td style="width:845px;"><strong><span class="fontColorRed">Note:</span></strong> <span class="fontColorThemeSecondary">Remember to not be too aggressive in reducing memory use because if the job runs out of memory it will fail, while if you request a smaller number of CPUs, your job will just be slower. If you are unsure of how much memory you need, it is generally better to start by overestimating and tighten your requests as you get a better understanding of your jobs through the statistics from sstat and sacct.</span><br></td></tr></tbody></table></div></div>
<p><span class="fontColorYellowLight"><strong>Which directory do I run my job from?</strong> </span><br>Remember, if you type, <strong><span class="fontColorBlueLight"><em>pwd –P</em> </span></strong>from the command line, it will tell you what directory you are currently in. If you wish to change directory, type:</p><pre><span class="fontColorBlueLight">cd &lt;filepath/directoryname&gt;</span></pre><p>It is preferable to run batch jobs in your projects folder or your scratch area. If you are unsure what file path your lab/project share is on, please contact the <a href="mailto:support@wehi.edu.au" data-cke-saved-href="mailto:support@wehi.edu.au" target="_blank" title="mailto:support@wehi.edu.au" data-interception="off"><span class="fontColorYellowLight">Helpdesk</span></a> who will let you know.</p>
