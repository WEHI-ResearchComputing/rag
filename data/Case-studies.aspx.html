<h2>Case studies</h2><p>The Research Computing team works with multiple teams and researchers within WEHI and assists&nbsp;with research computing design, digital data management, access to high-end computing and much more. Our aim is to help you produce your best research and advance science!</p><p>We have a team of experts whose skills will ably support and guide you through WEHI's extensive central storage to achieve the best route to success.</p><p>If you would like a better idea of the type of work, support and results we can provide, read some of the&nbsp;case studies&nbsp;we have documented below.</p><p>Otherwise, feel free to get in touch with us to ask questions, get advice, receive assistance, or even just to say hello!</p>
<h4 style="text-align:center;"><strong><span class="fontColorBlue">Benchmarking a Bayesian computation tool</span></strong></h4><p>Stefano Mangiola, in the Papenfuss lab, developed an R package to help identify outlier data in differential expression analysis.</p><p>These are very computationally expensive calculations which means that it is worth investing effort in understanding how different parameter choices and inputs affect execution time.</p><p>The Research Computing team was able to help by automating the parameter sweeps.</p><p>As Stefano modified his algorithms, we were easily able to rerun the benchmarks. We developed Python scripts to generate R scripts, submitted them to the batch system and then parsed results from the R output.</p><p>For more information read the subsequently published article in<br>NAR Genomics and Bioinformatics, Volume 3, Issue 1, March 2021, lqab005. Written by Stefano Mangiola, Evan A Thomas, Martin Modrak, Aki Vehtari, Anthony T Papenfuss, and&nbsp;titled:<br><a href="https://doi.org/10.1093/nargab/lqab005" data-cke-saved-href="https://doi.org/10.1093/nargab/lqab005" target="_blank" data-interception="off" title="https://doi.org/10.1093/nargab/lqab005">Probabilistic outlier identification for RNA sequencing generalised linear models</a><br>​​​​​​</p>
<h4 style="text-align:center;"><strong><span class="fontColorBlue">Download, process, delete for large projects</span></strong></h4><p>Due to idiosyncrasies of the way computing costs are paid for we can, in many, cases download data without a dollar-cost, whereas computing next to the data would be prohibitive.</p><p>For very large datasets this may introduce a storage problem, as WEHI does not have enough storage to hold some of these very large datasets. Large datasets also introduce an engineering problem. When downloading a relatively small dataset, it is reasonable to handle occasional failures manually. For large datasets, failures are inevitable and manual handling of errors becomes impractical.</p><p>We have created 2 repositories for handling downloads&nbsp;from Globus and GDC (TCGA). The software takes care of driving the query and download APIs, parallelisation, error handling, and retries. When a sample is downloaded a user workflow is called. If the workflow succeeds that sample can be deleted, thus decoupling the engineering problems and separating handling data from the process of analysing the data.</p><p><a href="https://github.com/WEHI-ResearchComputing/wehi-gdc" data-cke-saved-href="https://github.com/WEHI-ResearchComputing/wehi-gdc" target="_blank" data-interception="off" title="https://github.com/WEHI-ResearchComputing/wehi-gdc">github.com/WEHI-ResearchComputing/wehi-gdc</a><br><a href="https://github.com/WEHI-ResearchComputing/wehi-globus" data-cke-saved-href="https://github.com/WEHI-ResearchComputing/wehi-globus" target="_blank" data-interception="off" title="https://github.com/WEHI-ResearchComputing/wehi-globus">github.com/WEHI-ResearchComputing/wehi-globus</a><br>​​​​​​​</p>
<h4 style="text-align:center;"><strong><span class="fontColorBlue">Three thousand malaria genomes</span></strong></h4><p>To better understand the evolution of <em>plasmodium falciparum</em>, and in particular its development of drug resistance&nbsp;around the world, the Bahlo lab asked us to run a pipeline across 3000 WGS of samples.</p><p>They provided a <a href="http://docs.bpipe.org/" data-cke-saved-href="http://docs.bpipe.org/" target="_blank" data-interception="off" title="http://docs.bpipe.org/">Bpipe</a> pipeline as a template, however Bpipe does not scale to this amount of data.</p><p>We redeveloped the pipeline using <a href="https://toil.ucsc-cgl.org/" data-cke-saved-href="https://toil.ucsc-cgl.org/" target="_blank" data-interception="off" title="https://toil.ucsc-cgl.org/">Toil </a>which was the state-of-the-art workflow manager at the time. Toil did not support the Torque batch (which WEHI ran&nbsp;at the time) so we submitted a pull request to add support for traditional batch systems.</p><p>We were able to process ~3000 WGS samples through an 18-step workflow in a few weeks on Milton. The workflow was written in the Toil API and therefore is not of general interest but if you would like to look at it please contact us.</p>
<h4 style="text-align:center;"><strong><span class="fontColorBlue">The portable pipelines project</span></strong></h4><p>This project was developed out of the need to run (cancer) pipelines across WEHI, Peter Mac, and Melbourne Bioinformatics.</p><p>There are a plethora of pipeline tools but at that time most of the tools had not kept pace with the revolution in both the complexity of data and the complexity of compute environments.</p><p>New tools and initiatives, for example, Common Workflow Language (CWL), were being developed to address this complexity. However, it was clear this complexity was forcing tool developers to narrow their focus on an aspect of the pipeline domain. For example, CWL focused on providing a rigorous and complete description of a workflow but that the cost of ease of use and implementation support. Other tools were developed to interface with cloud and batch systems but at the expense of portability to other execution environments.</p><p>WEHI&nbsp;recognised there was a need for APIs that would allow different aspects of a pipeline system to be coupled together dynamically. This architecture greatly aids portability as it allows decoupling of the pipeline definition from specific infrastructure.</p><p>A proof of concept was funded by Tony Papenfuss. This architecture was adopted by the Portable Pipeline Group, funding was provided by Melbourne Bioinformatics&nbsp;and Peter Mac, and then the <strong>Janis Project</strong> was born.</p><p>Janis provides Python APIs for workflow definition, translation to transport formats (CWL, Broad’s WDL, and Nextflow currently implemented), and execution (SLURM, AWS, and GCP currently implemented). Specifying pipelines using the Python API is significantly easier than using current tools and pipelines have been demonstrated to run across precinct facilities and in the cloud.</p><p>Visit the Janis website at <strong><a href="https://janis.readthedocs.io/en/latest/" data-cke-saved-href="https://janis.readthedocs.io/en/latest/" target="_blank" data-interception="off" title="https://janis.readthedocs.io/en/latest/">janis.readthedocs.io/en/latest</a></strong></p>
